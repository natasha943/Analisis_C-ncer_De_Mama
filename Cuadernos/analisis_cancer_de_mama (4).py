# -*- coding: utf-8 -*-
"""Analisis_Cancer_De_Mama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16DQGbx7ekC5kschn1Z74G2DYkYk-JS8e

# Predictive Analysis of the Breast Cancer Dataset:

A Comprehensive Approach with PCA, KNN, and Neural Networks
 <center>
<img src="https://www.rededucom.org/img/proyectos/14/galeria/big/universidad-salesiana.jpg" width="200">

**Dataset:**  Breast Cancer Wisconsin Diagnostic Dataset (WDBC)

**Autor:** Erika Contreras, Jorge Pizarro


**UCI Dataset Usado: Breast Cancer Wisconsin (Diagnostic)** https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data

**DESCRIPTION:**

* GENERAL DESCRIPTION:
  - The dataset contains morphological measurements obtained from digitized images of breast tissue, used to diagnose tumors as Benign (1) or Malignant (0).
  - It allows breast cancer to be diagnosed in a non-invasive way.
  - It contains metrics obtained by computer, avoiding human bias.
  - It has highly informative features that facilitate model training with over
     95% accuracy.
  - It is an international standard for evaluating classification algorithms.


* DATASET STRUCTURE:
   - Total samples: 569 patients
   - Total variables: 30 numerical characteristics + 1 target variable
* CONSIDERATIONS:
   - The dataset presents natural outliers due to the clinical variability of tumors.
   - It is highly linearly separable, allowing for good
     performance in models such as KNN, PCA, and neural networks.

<h1>  1. LOADING THE DATASET
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import load_breast_cancer

# Cargar dataset
data = load_breast_cancer(as_frame=True)
df = data.frame

print("Shape:", df.shape)
df.head()

"""<h1> 2. STATISTICAL SUMMARY"""

df.describe().T

"""<h1>3. DATASET INFORMATION"""

print("\n=== 3. DATASET INFORMATION (df.info) ===")
df.info()

"""<h1>4. MISSING VALUES"""

print("\n=== 4. ANÁLISIS DE VALORES FALTANTES ===")
missing = df.isna().sum()
display(missing)

print("\nTotal de valores faltantes en todo el dataset:", int(missing.sum()))

"""<h1> 5. Distribution of the target variable"""

# Top 10 variables más correlacionadas con la clase (excluyendo 'target' de la lista)
top_corr = df.corr()['target'].abs().sort_values(ascending=False).head(11)
print("Top 10 variables más correlacionadas con 'target':")
print(top_corr.to_string())

# Gráfico de barras horizontales
plt.figure(figsize=(8, 6))
# Excluimos 'target' que siempre tendría correlación 1.0 consigo misma
top_corr_features = top_corr.drop('target') if 'target' in top_corr.index else top_corr
bars = plt.barh(top_corr_features.index, top_corr_features.values, color='salmon')
plt.title("Top 10 variables más correlacionadas con el diagnóstico (target)", fontsize=14)
plt.xlabel("Correlación absoluta", fontsize=12)
plt.ylabel("Variable", fontsize=12)
plt.gca().invert_yaxis()  # Para que la variable más correlacionada quede arriba
plt.grid(axis='x', alpha=0.3)

# Añadir valores en las barras
for bar in bars:
    width = bar.get_width()
    plt.text(width + 0.01, bar.get_y() + bar.get_height()/2,
             f'{width:.3f}', ha='left', va='center', fontsize=10)

plt.tight_layout()
plt.show()

# Seleccionar las características más relevantes (excluyendo 'target')
selected_features = top_corr.drop('target').index.tolist() if 'target' in top_corr.index else top_corr.index.tolist()
print(f"\nCaracterísticas seleccionadas ({len(selected_features)}): {selected_features}")

# Crear dataset reducido (incluyendo 'target' para análisis posterior)
df_reduced = df[selected_features + ['target']].copy()
print(f"\nDataset reducido - Dimensiones: {df_reduced.shape}")
print(f"Variables: {df_reduced.columns.tolist()}")

# Mostrar primeras filas del dataset reducido
print("\nPrimeras 5 filas del dataset reducido:")
display(df_reduced.head())

# Análisis adicional: correlación entre las características seleccionadas
print("\n=== ANÁLISIS ADICIONAL DE CARACTERÍSTICAS SELECCIONADAS ===")

# Matriz de correlación entre características seleccionadas
plt.figure(figsize=(10, 8))
corr_matrix_reduced = df_reduced.corr()
sns.heatmap(corr_matrix_reduced, annot=True, fmt=".2f", cmap="coolwarm",
            center=0, square=True, linewidths=0.5, cbar_kws={"shrink": 0.8})
plt.title("Matriz de correlación - Características seleccionadas", fontsize=14)
plt.tight_layout()
plt.show()

# Verificar multicolinealidad (correlaciones altas entre predictores)
print("\nCorrelaciones altas (>0.8) entre predictores seleccionados:")
high_corr_pairs = []
for i in range(len(selected_features)):
    for j in range(i+1, len(selected_features)):
        feat1, feat2 = selected_features[i], selected_features[j]
        corr_value = abs(df_reduced[feat1].corr(df_reduced[feat2]))
        if corr_value > 0.8:
            high_corr_pairs.append((feat1, feat2, corr_value))

if high_corr_pairs:
    for feat1, feat2, corr in high_corr_pairs:
        print(f"  {feat1} - {feat2}: {corr:.3f}")
else:
    print("  No hay correlaciones extremadamente altas (>0.8) entre predictores seleccionados.")

# Estadísticas descriptivas de las características seleccionadas
print("\nResumen estadístico de características seleccionadas:")
display(df_reduced[selected_features].describe().T)

# Distribución de las características seleccionadas por clase
print("\nValores promedio por clase para características seleccionadas:")
for feature in selected_features[:5]:  # Mostrar solo las primeras 5 para brevedad
    mean_by_class = df_reduced.groupby('target')[feature].mean()
    print(f"\n{feature}:")
    print(f"  Clase 0 (Maligno): {mean_by_class[0]:.3f}")
    print(f"  Clase 1 (Benigno): {mean_by_class[1]:.3f}")
    print(f"  Diferencia: {abs(mean_by_class[0] - mean_by_class[1]):.3f}")

# Guardar dataset reducido para uso posterior (opcional)
df_reduced.to_csv("breast_cancer_reduced_features.csv", index=False)
print(f"\nDataset reducido guardado como 'breast_cancer_reduced_features.csv'")

"""<h1> Interpretation <br>

* Variables related to size (radius, area, perimeter) and shape (concavity, concave points) are the most important for distinguishing between tumors.

* The reduced dataset retains the most discriminative characteristics
The 10 variables with the highest correlation + target were selected, obtaining:

  11 final columns

  569 rows (no sample loss)

* The selected features are highly correlated with each other
Extremely high correlations were observed between predictors.

* The statistical differences between classes are very large: the average values between malignant (0) and benign (1) show significant differences.

* The correlations between predictors are very high, showing structural redundancy useful for PCA.

<h1> 6. CORRELATION MATRIX
"""

print("\n=== 6. MATRIZ DE CORRELACIÓN ENTRE VARIABLES ===")
plt.figure(figsize=(12,10))
sns.heatmap(df.corr(), cmap="coolwarm")
plt.title("Matriz de correlación")
plt.show()

"""<h1> Interpretation <br>

The correlation matrix shows high multicollinearity between several characteristics of the dataset, especially between morphological variables related to tumor size and shape, such as radius, perimeter, area, and their mean, worst, and error versions. These variables show correlations greater than 0.8, indicating that they provide redundant information.

<h1> 7. VISUALIZACION DE HISTOGRAMAS / VARIABLES NUMÈRICAS
"""

print("\n=== 7. HISTOGRAMAS DE LAS VARIABLES NUMÉRICAS ===")

df.drop(columns=['target']).hist(bins=30, figsize=(16,14))
plt.suptitle("Distribución de variables numéricas", y=1.02)
plt.show()

"""<h1> Interpretation <br>

The histograms show that most of the numerical variables in the dataset have skewed distributions (right-skewed), especially those related to errors (“error”) and “worst” values. This indicates that there are many cases with small values and few with very large values.

 The main variables such as mean radius, mean perimeter, and mean area are closer to a normal distribution, but still show slight skewness. Overall, the histograms confirm the presence of very different scales and considerable variability.

<h1> 8. Boxplots for outliers
"""

print("\n=== 8. BOXPLOTS AND OUTLIER ANALYSIS ===")

plt.figure(figsize=(14, 8))
sns.boxplot(data=df.drop(columns=['target']))
plt.xticks(rotation=90)
plt.title("Boxplots por variable (sin incluir 'target')", fontsize=14)
plt.xlabel("Variables", fontsize=12)
plt.ylabel("Valores", fontsize=12)
plt.tight_layout()
plt.show()


df_numeric = df.drop(columns=['target'])

# Calcular límites usando IQR
Q1 = df_numeric.quantile(0.25)
Q3 = df_numeric.quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

print("\n" + "="*60)
print("ANÁLISIS DETALLADO DE OUTLIERS")
print("="*60)

# Contar outliers por variable
outliers_count = {}
outliers_percentage = {}

for col in df_numeric.columns:
    lower_col = lower_bound[col]
    upper_col = upper_bound[col]

    outliers_mask = (df_numeric[col] < lower_col) | (df_numeric[col] > upper_col)
    outliers_count[col] = outliers_mask.sum()
    outliers_percentage[col] = (outliers_mask.sum() / len(df_numeric)) * 100

# Crear DataFrame con resultados
outliers_df = pd.DataFrame({
    'Variable': list(outliers_count.keys()),
    'N_Outliers': list(outliers_count.values()),
    '%_Outliers': list(outliers_percentage.values())
})

print("\nTop 10 variables con más outliers:")
outliers_df_sorted = outliers_df.sort_values('N_Outliers', ascending=False)
display(outliers_df_sorted.head(10))

# Análisis general
total_outliers_detected = outliers_df['N_Outliers'].sum()
rows_with_any_outlier = ((df_numeric < lower_bound) | (df_numeric > upper_bound)).any(axis=1)
rows_affected = rows_with_any_outlier.sum()

print(f"\nEstadísticas generales:")
print(f"- Total outliers detectados: {total_outliers_detected}")
print(f"- Filas con al menos 1 outlier: {rows_affected} ({(rows_affected/len(df))*100:.1f}%)")

# Guardar dataset original para Fase 2
df.to_csv("breast_cancer_original.csv", index=False)
print("\n✓ Dataset guardado para Fase 2: 'breast_cancer_original.csv'")

"""<h1> Interpretation <br>

* Analysis using the IQR (Interquartile Range) method identified values that were extremely far from the central distribution of the WDBC dataset.
* The dataset contains a considerable number of outliers (608), representing 30.1% of the rows with at least one extreme value.

* The variables most affected are those related to errors and size measurements, which is consistent with their clinical nature.

<h1> 9. ANALYSIS BY CLASS (MALIGNANT VS BENIGN)
"""

print("\n" + "="*70)
print("ANÁLISIS POR CLASE: MALIGNO (0) vs BENIGNO (1)")
print("="*70)

# =============================================
# 1. SELECCIÓN DE VARIABLES CLAVE MÁS INFORMATIVAS
# =============================================
print("\n1. SELECCIÓN DE VARIABLES PARA ANÁLISIS")

# Basado en análisis de correlación anterior, seleccionamos las más relevantes
features_to_plot = [
    'mean radius',           # Tamaño del tumor
    'mean texture',          # Textura del tejido
    'mean perimeter',        # Perímetro del tumor
    'mean area',             # Área del tumor
    'mean concavity',        # Concavidad (indicador de malignidad)
    'mean concave points'    # Puntos cóncavos (importante según correlación)
]

print(f"   Analizando {len(features_to_plot)} variables clave:")
for i, feature in enumerate(features_to_plot, 1):
    print(f"   {i}. {feature}")

# =============================================
# 2. ANÁLISIS ESTADÍSTICO POR CLASE
# =============================================
print("\n2. ESTADÍSTICAS DESCRIPTIVAS POR CLASE")
print("   " + "-"*50)

# Crear DataFrame con estadísticas comparativas
stats_comparison = []

for feature in features_to_plot:
    if feature in df.columns:
        # Estadísticas para clase 0 (Maligno)
        stats_0 = df[df['target'] == 0][feature].describe()
        # Estadísticas para clase 1 (Benigno)
        stats_1 = df[df['target'] == 1][feature].describe()

        stats_comparison.append({
            'Variable': feature,
            'Maligno_Media': stats_0['mean'],
            'Maligno_Std': stats_0['std'],
            'Benigno_Media': stats_1['mean'],
            'Benigno_Std': stats_1['std'],
            'Diferencia_Media': stats_0['mean'] - stats_1['mean'],
            'Diferencia_%': ((stats_0['mean'] - stats_1['mean']) / stats_1['mean']) * 100
        })

# Convertir a DataFrame y mostrar
stats_df = pd.DataFrame(stats_comparison)
print("\n   Comparación de medias por clase:")
display(stats_df[['Variable', 'Maligno_Media', 'Benigno_Media', 'Diferencia_Media', 'Diferencia_%']])

# =============================================
# 3. VISUALIZACIÓN MEJORADA: SUBPLOTS ORGANIZADOS
# =============================================
print("\n3. VISUALIZACIÓN COMPARATIVA")

# Crear figura con subplots organizados
fig = plt.figure(figsize=(16, 12))

# Título principal
plt.suptitle('ANÁLISIS POR CLASE: MALIGNO vs BENIGNO', fontsize=16, y=1.02)

# BOXPLOTS (fila superior)
print("\n   Boxplots por clase (diferencias de distribución):")
for idx, feature in enumerate(features_to_plot[:3], 1):  # Primeras 3
    ax = plt.subplot(3, 3, idx)

    # Crear boxplot con mejor estilo
    boxprops = dict(linestyle='-', linewidth=1.5)
    medianprops = dict(linestyle='-', linewidth=2.5, color='red')
    whiskerprops = dict(linestyle='--', linewidth=1.5)

    bp = ax.boxplot([
        df[df['target'] == 0][feature].dropna(),
        df[df['target'] == 1][feature].dropna()
    ],
    labels=['Maligno (0)', 'Benigno (1)'],
    patch_artist=True,
    boxprops=boxprops,
    medianprops=medianprops,
    whiskerprops=whiskerprops)

    # Colorear las cajas
    colors = ['#FF6B6B', '#4ECDC4']  # Rojo para maligno, verde agua para benigno
    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)

    ax.set_title(feature, fontsize=12, fontweight='bold')
    ax.set_ylabel('Valor', fontsize=10)
    ax.grid(True, alpha=0.3, axis='y')

    # Añadir anotación con diferencia de medias
    mean_diff = stats_df.loc[stats_df['Variable'] == feature, 'Diferencia_Media'].values[0]
    ax.text(0.5, 0.95, f'Δ = {mean_diff:.2f}',
            transform=ax.transAxes, ha='center', va='top',
            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))

# HISTOGRAMAS SUPERPUESTOS (fila media)
print("\n   Histogramas superpuestos (distribuciones):")
for idx, feature in enumerate(features_to_plot[3:6], 4):  # Siguientes 3
    ax = plt.subplot(3, 3, idx)

    # Histograma para clase 0 (Maligno)
    ax.hist(df[df['target'] == 0][feature].dropna(),
            bins=30, alpha=0.6, color='#FF6B6B',
            label='Maligno (0)', density=True)

    # Histograma para clase 1 (Benigno)
    ax.hist(df[df['target'] == 1][feature].dropna(),
            bins=30, alpha=0.6, color='#4ECDC4',
            label='Benigno (1)', density=True)

    ax.set_title(feature, fontsize=12, fontweight='bold')
    ax.set_xlabel('Valor', fontsize=10)
    ax.set_ylabel('Densidad', fontsize=10)
    ax.legend(fontsize=9)
    ax.grid(True, alpha=0.3)

    # Añadir líneas de media
    mean_0 = df[df['target'] == 0][feature].mean()
    mean_1 = df[df['target'] == 1][feature].mean()

    ax.axvline(mean_0, color='#FF6B6B', linestyle='--', linewidth=2, alpha=0.8)
    ax.axvline(mean_1, color='#4ECDC4', linestyle='--', linewidth=2, alpha=0.8)

# VIOLIN PLOTS (fila inferior)
print("\n   Violin plots (distribución + densidad):")
for idx, feature in enumerate(features_to_plot[3:6], 7):  # Últimas 3 (usando diferentes)
    ax = plt.subplot(3, 3, idx)

    # Crear violin plot
    violin_data = [df[df['target'] == 0][feature].dropna(),
                   df[df['target'] == 1][feature].dropna()]

    vp = ax.violinplot(violin_data, showmeans=True, showmedians=True)

    # Personalizar violines
    colors = ['#FF6B6B', '#4ECDC4']
    for i, pc in enumerate(vp['bodies']):
        pc.set_facecolor(colors[i])
        pc.set_alpha(0.7)
        pc.set_edgecolor('black')

    # Personalizar estadísticas
    vp['cmeans'].set_color('yellow')
    vp['cmedians'].set_color('black')
    vp['cbars'].set_color('black')
    vp['cmins'].set_color('black')
    vp['cmaxes'].set_color('black')

    ax.set_xticks([1, 2])
    ax.set_xticklabels(['Maligno', 'Benigno'], fontsize=10)
    ax.set_title(feature, fontsize=12, fontweight='bold')
    ax.set_ylabel('Valor', fontsize=10)
    ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

# =============================================
# 4. PRUEBAS ESTADÍSTICAS DE DIFERENCIAS
# =============================================
print("\n4. PRUEBAS ESTADÍSTICAS DE SIGNIFICANCIA")
print("   " + "-"*50)

from scipy import stats

print("\n   Prueba t-Student para diferencias de medias:")
print("   " + "-"*50)
print(f"{'Variable':<25} {'t-statistic':<15} {'p-value':<15} {'Significativo (p<0.05)':<20}")
print("   " + "-"*50)

for feature in features_to_plot:
    if feature in df.columns:
        # Datos por clase
        data_0 = df[df['target'] == 0][feature].dropna()
        data_1 = df[df['target'] == 1][feature].dropna()

        # Prueba t de Student (asumiendo varianzas diferentes)
        t_stat, p_value = stats.ttest_ind(data_0, data_1, equal_var=False)

        # Determinar significancia
        significativo = "SÍ" if p_value < 0.05 else "NO"

        print(f"{feature:<25} {t_stat:>10.3f}    {p_value:>12.4f}    {significativo:^20}")

# =============================================
# 5. ANÁLISIS DE SOLAPAMIENTO (OVERLAP)
# =============================================
print("\n5. ANÁLISIS DE SOLAPAMIENTO ENTRE CLASES")
print("   " + "-"*50)

print("\n   Variables con menor solapamiento (mejores para clasificación):")

overlap_analysis = []
for feature in features_to_plot:
    if feature in df.columns:
        # Calcular rango de valores
        range_0 = df[df['target'] == 0][feature].agg(['min', 'max'])
        range_1 = df[df['target'] == 1][feature].agg(['min', 'max'])

        # Calcular solapamiento
        overlap_min = max(range_0['min'], range_1['min'])
        overlap_max = min(range_0['max'], range_1['max'])

        if overlap_max > overlap_min:
            overlap_range = overlap_max - overlap_min
            total_range = max(range_0['max'], range_1['max']) - min(range_0['min'], range_1['min'])
            overlap_percentage = (overlap_range / total_range) * 100
        else:
            overlap_percentage = 0

        overlap_analysis.append({
            'Variable': feature,
            'Solapamiento_%': overlap_percentage,
            'Rango_Maligno': f"{range_0['min']:.1f}-{range_0['max']:.1f}",
            'Rango_Benigno': f"{range_1['min']:.1f}-{range_1['max']:.1f}"
        })

# Ordenar por menor solapamiento (mejor para clasificación)
overlap_df = pd.DataFrame(overlap_analysis).sort_values('Solapamiento_%')
display(overlap_df)

# =============================================
# 6. VISUALIZACIÓN DE SEPARACIÓN IDEAL
# =============================================
print("\n6. VISUALIZACIÓN: MEJORES VARIABLES PARA CLASIFICACIÓN")

# Seleccionar las 2 variables con menor solapamiento
best_features = overlap_df.head(2)['Variable'].tolist()

if len(best_features) >= 2:
    fig, ax = plt.subplots(1, 2, figsize=(14, 5))

    for idx, feature in enumerate(best_features):
        # Scatter plot con más transparencia para ver densidad
        ax[idx].scatter(df[df['target'] == 0][feature],
                       np.random.normal(0, 0.05, size=len(df[df['target'] == 0])),
                       alpha=0.6, color='#FF6B6B', s=30, label='Maligno')

        ax[idx].scatter(df[df['target'] == 1][feature],
                       np.random.normal(1, 0.05, size=len(df[df['target'] == 1])),
                       alpha=0.6, color='#4ECDC4', s=30, label='Benigno')

        ax[idx].set_xlabel(feature, fontsize=11)
        ax[idx].set_yticks([0, 1])
        ax[idx].set_yticklabels(['Maligno', 'Benigno'], fontsize=10)
        ax[idx].set_title(f'{feature}\n(Solapamiento: {overlap_df.loc[overlap_df["Variable"]==feature, "Solapamiento_%"].values[0]:.1f}%)',
                         fontsize=12, fontweight='bold')
        ax[idx].legend(fontsize=9)
        ax[idx].grid(True, alpha=0.3)

    plt.suptitle('Variables con MEJOR Separación entre Clases', fontsize=14, y=1.05)
    plt.tight_layout()
    plt.show()

"""<h1> CONCLUSIONS

Malignant tumors have significantly higher values for mean radius, mean texture, mean perimeter, mean area, mean concavity, and mean concave points.

The differences (Δ) show a clear separation between the two classes, especially in terms of size and shape measurements.

The histograms and violin plots show that the distributions of both classes overlap little, indicating that these variables are highly useful for distinguishing between benign and malignant tumors.

Overall, the graph confirms that the morphological characteristics of breast tissue contain very marked patterns that allow both classes to be differentiated with high precision.

<h1> 10. SAVE THE CLEAN DATASET
"""

clean = df.copy()
clean.to_csv("Examen_Listo_Fase1.csv", index=False)
print("Dataset limpio guardado como Examen_Listo_Fase1.csv")

"""# New section

<h1> PHASE 2 — MODELS, OPTIMIZATION, AND EVALUATION

<h1> Import Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report,
    mean_absolute_error, mean_squared_error, r2_score
)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Dense, Dropout, Input

"""<h1> 1. Load clean dataset"""

import pandas as pd
df = pd.read_csv("Examen_Listo_Fase1.csv")
X = df.drop(columns=['target'])
y = df['target']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
   X, y, test_size=0.20, random_state=42, stratify=y)
print("Dataset Listo")

"""<h1> 2. Split Train–Test and Escalation"""

from sklearn.preprocessing import StandardScaler

print("\n=== 2. CLASIFICACIÓN – PREPARACIÓN Y ESCALADO ===")

# Usamos el MISMO split global definido antes
Xc_train = X_train.copy()
Xc_test  = X_test.copy()
yc_train = y_train.copy()
yc_test  = y_test.copy()

print("Xc_train:", Xc_train.shape, "Xc_test:", Xc_test.shape)

# Estandarización
scaler_clf = StandardScaler()
Xc_train_scaled = scaler_clf.fit_transform(Xc_train)
Xc_test_scaled  = scaler_clf.transform(Xc_test)

"""<h1>Interpretation <br>


The division of the dataset produced 455 samples for training and 114 for testing, maintaining the 30 predictor variables in both sets. This separation allows the model to be trained without bias and its generalization ability to be evaluated objectively.<br>

This indicates that a typical 80%–20% train-test split was correctly applied:

80% for training → 455 records

20% for testing → 114 records

This prevents overfitting and allows the model's actual ability to generalize to be measured.

<h1> 3. Preprocessing
"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train)
X_test_s = scaler.transform(X_test)

"""<h1> 4. Modelos baseline"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score

logreg = LogisticRegression(max_iter=500)
logreg.fit(X_train_s, y_train)

pred = logreg.predict(X_test_s)
probs = logreg.predict_proba(X_test_s)[:,1]

print("Accuracy:", accuracy_score(y_test, pred))
print("F1:", f1_score(y_test, pred))
print("AUC:", roc_auc_score(y_test, probs))

"""<h1> 5. PCA FOR CLASSIFICATION (DIMENSION REDUCTION)"""

from sklearn.decomposition import PCA
import numpy as np
import matplotlib.pyplot as plt

print("\n=== 3. PCA PARA CLASIFICACIÓN ===")

# PCA completo para estudiar varianza
pca_tmp = PCA(n_components=None, random_state=42)
pca_tmp.fit(Xc_train_scaled)

var_exp = np.cumsum(pca_tmp.explained_variance_ratio_)
n_comp_95 = np.argmax(var_exp >= 0.95) + 1
print(f"Número de componentes para explicar ~95% varianza: {n_comp_95}")

plt.figure(figsize=(6,4))
plt.plot(var_exp, marker="o")
plt.axhline(0.95, linestyle="--")
plt.xlabel("Número de componentes")
plt.ylabel("Varianza explicada acumulada")
plt.title("PCA – Varianza acumulada (clasificación)")
plt.grid(True)
plt.show()

# PCA definitivo
pca_clf = PCA(n_components=n_comp_95, random_state=42)
Xc_train_pca = pca_clf.fit_transform(Xc_train_scaled)
Xc_test_pca  = pca_clf.transform(Xc_test_scaled)

print("Xc_train_pca:", Xc_train_pca.shape, "Xc_test_pca:", Xc_test_pca.shape)

"""<h1> Interpretation: <br>

PCA is highly effective in this dataset. With only 10 components, most of the relevant information (95%) is preserved, significantly reducing the dimensionality compared to the original 30 variables.

This improves computational efficiency and reduces redundancy between highly correlated features.

<h1> 6. CLASSIFICATION – MODEL 1: KNN (ORIGINAL + PCA)
"""

print("\n=== 6. CLASIFICACIÓN – MODELO 1: KNN ===")

param_grid_knn = {
    "n_neighbors": [3,5,7,9,11],
    "weights": ["uniform", "distance"],
    "metric": ["euclidean", "manhattan"]
}

# ---- KNN sobre datos ESCALADOS (sin PCA) ----
knn_clf = KNeighborsClassifier()
grid_knn_clf = GridSearchCV(
    knn_clf, param_grid_knn,
    cv=5, scoring="accuracy", n_jobs=-1
)
grid_knn_clf.fit(Xc_train_scaled, yc_train)

print("Mejores parámetros KNN (sin PCA):", grid_knn_clf.best_params_)
best_knn_clf = grid_knn_clf.best_estimator_

yc_pred_knn = best_knn_clf.predict(Xc_test_scaled)
acc_knn = accuracy_score(yc_test, yc_pred_knn)
print("Accuracy KNN (sin PCA):", acc_knn)

# ---- KNN sobre datos PCA ----
knn_clf_pca = KNeighborsClassifier()
grid_knn_clf_pca = GridSearchCV(
    knn_clf_pca, param_grid_knn,
    cv=5, scoring="accuracy", n_jobs=-1
)
grid_knn_clf_pca.fit(Xc_train_pca, yc_train)

print("Mejores parámetros KNN (con PCA):", grid_knn_clf_pca.best_params_)
best_knn_clf_pca = grid_knn_clf_pca.best_estimator_

yc_pred_knn_pca = best_knn_clf_pca.predict(Xc_test_pca)
acc_knn_pca = accuracy_score(yc_test, yc_pred_knn_pca)
print("Accuracy KNN (con PCA):", acc_knn_pca)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

print("\n=== 6. CLASIFICACIÓN – MODELO 1: KNN ===")

param_grid_knn = {
    "n_neighbors": [3,5,7,9,11],
    "weights": ["uniform", "distance"],
    "metric": ["euclidean", "manhattan"]
}

# ============================================================
# 6.1 KNN SIN PCA (datos escalados)
# ============================================================
knn_clf = KNeighborsClassifier()
grid_knn_clf = GridSearchCV(
    knn_clf, param_grid_knn,
    cv=5, scoring="accuracy", n_jobs=-1
)
grid_knn_clf.fit(Xc_train_scaled, yc_train)

best_knn_clf = grid_knn_clf.best_estimator_
yc_pred_knn = best_knn_clf.predict(Xc_test_scaled)

# Métricas sin PCA
acc_knn  = accuracy_score(yc_test, yc_pred_knn)
prec_knn = precision_score(yc_test, yc_pred_knn)
rec_knn  = recall_score(yc_test, yc_pred_knn)
f1_knn   = f1_score(yc_test, yc_pred_knn)

cm_knn = confusion_matrix(yc_test, yc_pred_knn)

# ============================================================
# 6.2 KNN CON PCA
# ============================================================
knn_clf_pca = KNeighborsClassifier()
grid_knn_clf_pca = GridSearchCV(
    knn_clf_pca, param_grid_knn,
    cv=5, scoring="accuracy", n_jobs=-1
)
grid_knn_clf_pca.fit(Xc_train_pca, yc_train)

best_knn_clf_pca = grid_knn_clf_pca.best_estimator_
yc_pred_knn_pca = best_knn_clf_pca.predict(Xc_test_pca)

# Métricas con PCA
acc_knn_pca  = accuracy_score(yc_test, yc_pred_knn_pca)
prec_knn_pca = precision_score(yc_test, yc_pred_knn_pca)
rec_knn_pca  = recall_score(yc_test, yc_pred_knn_pca)
f1_knn_pca   = f1_score(yc_test, yc_pred_knn_pca)

cm_knn_pca = confusion_matrix(yc_test, yc_pred_knn_pca)

# ============================================================
# 6.3 TABLA AUTOMÁTICA DE RESULTADOS
# ============================================================
tabla_knn = pd.DataFrame({
    "Modelo": ["KNN (sin PCA)", "KNN (con PCA)"],
    "Accuracy": [acc_knn, acc_knn_pca],
    "Precision": [prec_knn, prec_knn_pca],
    "Recall": [rec_knn, rec_knn_pca],
    "F1-Score": [f1_knn, f1_knn_pca],
    "Mejores Parámetros": [
        str(grid_knn_clf.best_params_),
        str(grid_knn_clf_pca.best_params_)
    ]
})

print("\n=== TABLA DE MÉTRICAS KNN ===")
display(tabla_knn)

# ============================================================
# 6.4 MATRICES DE CONFUSIÓN VISUALES
# ============================================================
plt.figure(figsize=(12,5))

# ------------------ KNN SIN PCA ------------------
plt.subplot(1,2,1)
sns.heatmap(cm_knn, annot=True, fmt="d", cmap="Blues")
plt.title("KNN – Sin PCA")
plt.xlabel("Predicción")
plt.ylabel("Real")

# ------------------ KNN CON PCA ------------------
plt.subplot(1,2,2)
sns.heatmap(cm_knn_pca, annot=True, fmt="d", cmap="Oranges")
plt.title("KNN – Con PCA")
plt.xlabel("Predicción")
plt.ylabel("Real")

plt.tight_layout()
plt.show()

"""***Interpretation:***

The confusion matrix shows that the KNN model performs very similarly with and without PCA. Without PCA, the model correctly classifies 112 out of 114 cases, while with PCA it correctly classifies 109 out of 114. This indicates that dimensionality reduction maintains virtually the same classification ability, although it introduces a slight loss of accuracy. Even so, PCA remains a valid option when seeking to reduce complexity or improve computational efficiency without significantly sacrificing performance.

<h1> 7. CLASSIFICATION – MODEL 2: NEURAL NETWORK (KERAS)

---
"""

print("\n=== 7. CLASIFICACIÓN – MODELO 2: RED NEURONAL (KERAS) ===")

def build_nn_classifier(input_dim, n_hidden1=64, n_hidden2=32, lr=0.001, dropout_rate=0.2):
    model = Sequential()
    model.add(Input(shape=(input_dim,)))
    model.add(Dense(n_hidden1, activation="relu"))
    model.add(Dropout(dropout_rate))
    model.add(Dense(n_hidden2, activation="relu"))
    model.add(Dropout(dropout_rate))
    model.add(Dense(1, activation="sigmoid"))
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
    model.compile(
        loss="binary_crossentropy",
        optimizer=optimizer,
        metrics=["accuracy"]
    )
    return model

early_stop = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)

# ---------- Entrenamiento NN en datos ESCALADOS ----------
input_dim_full = Xc_train_scaled.shape[1]
nn_clf_full = build_nn_classifier(input_dim=input_dim_full, n_hidden1=64, n_hidden2=32, lr=0.001)

history_full = nn_clf_full.fit(
    Xc_train_scaled, yc_train,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[early_stop],
    verbose=0
)

yc_pred_proba_full = nn_clf_full.predict(Xc_test_scaled).ravel()
yc_pred_nn_full = (yc_pred_proba_full >= 0.5).astype(int)

acc_nn_full = accuracy_score(yc_test, yc_pred_nn_full)
print("Accuracy NN (sin PCA):", acc_nn_full)

# ---------- Entrenamiento NN en datos PCA ----------
input_dim_pca = Xc_train_pca.shape[1]
nn_clf_pca = build_nn_classifier(input_dim=input_dim_pca, n_hidden1=32, n_hidden2=16, lr=0.001)

history_pca = nn_clf_pca.fit(
    Xc_train_pca, yc_train,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[early_stop],
    verbose=0
)

yc_pred_proba_pca = nn_clf_pca.predict(Xc_test_pca).ravel()
yc_pred_nn_pca = (yc_pred_proba_pca >= 0.5).astype(int)

acc_nn_pca = accuracy_score(yc_test, yc_pred_nn_pca)
print("Accuracy NN (con PCA):", acc_nn_pca)

"""***Interpretation:***

0% loss of accuracy when applying PCA (both models obtain 96.49%).

The neural network correctly classifies 96 out of every 100 patients.

PCA reduces dimensionality but maintains the same performance as the original model.

The two versions of the model show identical performance, indicating that the essential information in the dataset is preserved even after feature reduction.

<h1> 8. REGRESSION – PREPARATION
"""

print("\n=== 8. REGRESIÓN – PREPARACIÓN ===")

# Elegimos una variable continua como objetivo de regresión
target_reg = "mean area"
y_reg = df[target_reg]
X_reg = df.drop(columns=["target", target_reg])

Xr_train, Xr_test, yr_train, yr_test = train_test_split(
    X_reg, y_reg, test_size=0.2, random_state=42
)

print("Xr_train:", Xr_train.shape, "Xr_test:", Xr_test.shape)

# -------------------------------------------------------
# 8.1 Descripción básica de la variable de regresión
# -------------------------------------------------------
print("\nDescripción estadística de la variable objetivo (mean area):")
display(yr_train.describe().to_frame(name=target_reg))

# Correlación de cada variable con la variable objetivo
corr_reg = df.drop(columns=["target"]).corr()[target_reg].sort_values(ascending=False)
top_corr_reg = corr_reg.head(10).to_frame(name="Correlación con 'mean area'")
print("\nTop 10 variables más correlacionadas con la variable objetivo:")
display(top_corr_reg)

# -------------------------------------------------------
# 8.2 Escalado
# -------------------------------------------------------
scaler_reg = StandardScaler()
Xr_train_scaled = scaler_reg.fit_transform(Xr_train)
Xr_test_scaled  = scaler_reg.transform(Xr_test)

# -------------------------------------------------------
# 8.3 PCA para regresión (95% varianza)
# -------------------------------------------------------
pca_reg_full = PCA(n_components=None, random_state=42)
pca_reg_full.fit(Xr_train_scaled)

var_exp_reg = pca_reg_full.explained_variance_ratio_
var_exp_reg_cum = np.cumsum(var_exp_reg)
n_comp_reg_95 = np.argmax(var_exp_reg_cum >= 0.95) + 1
print(f"\nComponentes PCA para regresión (~95% varianza): {n_comp_reg_95}")

# Tabla de varianza explicada por componente
tabla_pca_reg = pd.DataFrame({
    "Componente": np.arange(1, len(var_exp_reg) + 1),
    "Varianza explicada": var_exp_reg,
    "Varianza acumulada": var_exp_reg_cum
})
print("\nVarianza explicada por cada componente (regresión):")
display(tabla_pca_reg.head(10))  # primeras 10 filas para no saturar

# Gráfica de varianza acumulada
plt.figure(figsize=(6,4))
plt.plot(var_exp_reg_cum, marker="o")
plt.axhline(0.95, linestyle="--")
plt.xlabel("Número de componentes")
plt.ylabel("Varianza explicada acumulada")
plt.title("PCA – Varianza acumulada (regresión)")
plt.grid(True)
plt.show()

# PCA definitivo con n_comp_reg_95
pca_reg = PCA(n_components=n_comp_reg_95, random_state=42)
Xr_train_pca = pca_reg.fit_transform(Xr_train_scaled)
Xr_test_pca  = pca_reg.transform(Xr_test_scaled)

print("Xr_train_pca:", Xr_train_pca.shape, "Xr_test_pca:", Xr_test_pca.shape)

# -------------------------------------------------------
# 8.4 (Opcional) Visualización 2D de los primeros componentes
# -------------------------------------------------------
if n_comp_reg_95 >= 2:
    plt.figure(figsize=(6,5))
    scatter = plt.scatter(
        Xr_train_pca[:, 0],
        Xr_train_pca[:, 1],
        c=yr_train,
        cmap="viridis",
        alpha=0.7
    )
    plt.colorbar(scatter, label=target_reg)
    plt.xlabel("PC1")
    plt.ylabel("PC2")
    plt.title("Regresión – Proyección PCA (PC1 vs PC2)")
    plt.grid(True)
    plt.show()

"""***Interpretation:***

Regression preparation reveals that mean area is highly correlated with tumor size measurements, and that PCA allows for a reduction from 29 to 10 dimensions while retaining 95% of the information essential for modeling.

:<h1> 9. REGression – KNN REGRESSOR + NN KERAS
"""

print("\n=== 9. REGRESIÓN – MODELOS ===")

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Función auxiliar para métricas de regresión

def regression_metrics(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)
    return mae, mse, rmse, r2

# ---- 9.1 KNN REGRESSOR SIN PCA ----
knn_reg = KNeighborsRegressor(
    n_neighbors=5,
    weights="distance",
    metric="euclidean"
)
knn_reg.fit(Xr_train_scaled, yr_train)
yr_pred_knn = knn_reg.predict(Xr_test_scaled)
print("KNN Regressor SIN PCA entrenado.")

# ---- 9.2 KNN REGRESSOR CON PCA ----
knn_reg_pca = KNeighborsRegressor(
    n_neighbors=5,
    weights="distance",
    metric="euclidean"
)
knn_reg_pca.fit(Xr_train_pca, yr_train)
yr_pred_knn_pca = knn_reg_pca.predict(Xr_test_pca)
print("KNN Regressor CON PCA entrenado.")

# ---- 9.3 RED NEURONAL REGRESORA SIN PCA (KERAS) ----
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.callbacks import EarlyStopping

def build_nn_regressor(input_dim, n_hidden1=64, n_hidden2=32, lr=0.001, dropout_rate=0.2):
    model = Sequential()
    model.add(Input(shape=(input_dim,)))          # capa de entrada
    model.add(Dense(n_hidden1, activation="relu"))
    model.add(Dropout(dropout_rate))
    model.add(Dense(n_hidden2, activation="relu"))
    model.add(Dropout(dropout_rate))
    model.add(Dense(1, activation="linear"))      # salida continua
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
    model.compile(
        loss="mse",
        optimizer=optimizer,
        metrics=["mae"]
    )
    return model

early_stop_reg = EarlyStopping(
    monitor="val_loss",
    patience=10,
    restore_best_weights=True
)

input_dim_reg_full = Xr_train_scaled.shape[1]
nn_reg_full = build_nn_regressor(
    input_dim=input_dim_reg_full,
    n_hidden1=64,
    n_hidden2=32,
    lr=0.001,
    dropout_rate=0.2
)

history_reg_full = nn_reg_full.fit(
    Xr_train_scaled, yr_train,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[early_stop_reg],
    verbose=0
)

yr_pred_nn_full = nn_reg_full.predict(Xr_test_scaled).ravel()
print("Red neuronal regresora SIN PCA entrenada.")

# ---- 9.4 RED NEURONAL REGRESORA CON PCA (KERAS) ----
input_dim_reg_pca = Xr_train_pca.shape[1]
nn_reg_pca = build_nn_regressor(
    input_dim=input_dim_reg_pca,
    n_hidden1=32,
    n_hidden2=16,
    lr=0.001,
    dropout_rate=0.2
)

history_reg_pca = nn_reg_pca.fit(
    Xr_train_pca, yr_train,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[early_stop_reg],
    verbose=0
)

yr_pred_nn_pca = nn_reg_pca.predict(Xr_test_pca).ravel()
print("Red neuronal regresora CON PCA entrenada.")


# 10. GRÁFICAS REAL vs PREDICHO

plt.figure(figsize=(12,10))

model_names = ["KNN (sin PCA)", "KNN (con PCA)", "NN (sin PCA)", "NN (con PCA)"]
preds = [yr_pred_knn, yr_pred_knn_pca, yr_pred_nn_full, yr_pred_nn_pca]

for i, (name, y_pred) in enumerate(zip(model_names, preds), 1):
    plt.subplot(2, 2, i)
    plt.scatter(yr_test, y_pred, alpha=0.7)
    plt.plot([yr_test.min(), yr_test.max()],
             [yr_test.min(), yr_test.max()],
             "r--", linewidth=2)
    plt.xlabel("Valor real (mean area)")
    plt.ylabel("Valor predicho")
    plt.title(name)
    plt.grid(True)

plt.tight_layout()
plt.show()

"""<h1> Analysis of the graphs <br>

Visual analysis of the four scatter plots reveals that both the nearest neighbor algorithm (KNN) and neural networks (NN) are highly effective for the regression task of predicting the “mean area,” demonstrating excellent accuracy and a strong linear correlation between actual and predicted values.

It is noteworthy that the inclusion of Principal Component Analysis (PCA), a common technique for dimensionality reduction, had minimal impact on the performance of both models.

Thus, the four configurations evaluated (KNN/NN with or without PCA) are equally robust and reliable for this specific application.

<h1> 10. REGESSION METRICS
"""

# 10. MÉTRICAS Y TABLA DE RESULTADOS
print("\n=== 10. MÉTRICAS DE REGRESIÓN ===")

mae_knn, mse_knn, rmse_knn, r2_knn = regression_metrics(yr_test, yr_pred_knn)
mae_knn_pca, mse_knn_pca, rmse_knn_pca, r2_knn_pca = regression_metrics(yr_test, yr_pred_knn_pca)
mae_nn, mse_nn, rmse_nn, r2_nn = regression_metrics(yr_test, yr_pred_nn_full)
mae_nn_pca, mse_nn_pca, rmse_nn_pca, r2_nn_pca = regression_metrics(yr_test, yr_pred_nn_pca)

tabla_reg = pd.DataFrame({
    "Modelo": [
        "KNN (sin PCA)",
        "KNN (con PCA)",
        "NN (sin PCA)",
        "NN (con PCA)"
    ],
    "MAE":  [mae_knn, mae_knn_pca, mae_nn, mae_nn_pca],
    "MSE":  [mse_knn, mse_knn_pca, mse_nn, mse_nn_pca],
    "RMSE": [rmse_knn, rmse_knn_pca, rmse_nn, rmse_nn_pca],
    "R2":   [r2_knn, r2_knn_pca, r2_nn, r2_nn_pca]
})

print("\n=== TABLA DE MÉTRICAS DE REGRESIÓN ===")
display(tabla_reg)

"""***Interpretation:***

The best model for regression is KNN with PCA, achieving the lowest error (MAE = 56.45) and the highest explanatory power (R² = 0.9545), while the worst performance is presented by the neural network with PCA.

<h1> 11. SUMMARY COMPARISON OF MODELS CLASSIFICATION
"""

print("\n=== 11. COMPARACIÓN RESUMIDA – CLASIFICACIÓN (Accuracy) ===")
res_clf = pd.DataFrame({
    "Modelo": ["KNN", "KNN + PCA", "NN Keras", "NN Keras + PCA"],
    "Accuracy": [acc_knn, acc_knn_pca, acc_nn_full, acc_nn_pca]
})
display(res_clf)

"""<h1>Interpretation:

The classification results show that the KNN model without PCA performed best with an accuracy of 0.982456 (98.25%), correctly classifying almost 98 out of every 100 cases, confirming that the original structure of the dataset favors this distance-based algorithm.

 When applying PCA, the performance of KNN decreases to 0.956140 (95.61%), showing a loss of 2.64%, since the reduction in dimensionality alters the geometry of the data essential for this method.

 The neural network achieved an accuracy of 0.964912 (96.49%) both with and without PCA, showing stability and robustness when working in a reduced space without significant loss of information.

 In summary, KNN without PCA is the most accurate model, while PCA negatively affects KNN but does not modify the performance of the neural network, which remains constant at around 96.5%.

<h1> 13. PREDICTION WITH NEW EXAMPLES
"""

print("\n=== 13. PREDICCIÓN CON NUEVOS EJEMPLOS ===")

# Un ejemplo benigno (target = 1) y uno maligno (target = 0) del test
ejemplo_benigno = Xc_test[yc_test == 1].iloc[0:1]
ejemplo_maligno = Xc_test[yc_test == 0].iloc[0:1]

print("\nEjemplo benigno (clase real = 1):")
display(ejemplo_benigno)

print("\nEjemplo maligno (clase real = 0):")
display(ejemplo_maligno)

def predecir_ejemplo(df_ejemplo, scaler, modelo_knn, modelo_nn):
    """
    df_ejemplo : DataFrame con UNA fila (un paciente)
    scaler     : StandardScaler ya ajustado en entrenamiento
    modelo_knn : mejor modelo KNN de clasificación
    modelo_nn  : red neuronal de clasificación (salida sigmoide)
    """
    # Escalamos igual que en entrenamiento
    x_scaled = scaler.transform(df_ejemplo)

    # Predicción KNN (clase 0/1)
    pred_knn = int(modelo_knn.predict(x_scaled)[0])

    # Red neuronal devuelve probabilidad de clase 1 (BENIGNO)
    proba_benigno = float(modelo_nn.predict(x_scaled).ravel()[0])
    pred_nn = int(proba_benigno >= 0.5)

    return pred_knn, pred_nn, proba_benigno

# --------------------- Predicción para ejemplo BENIGNO (modelos sin PCA) ---
pk_knn_b, pk_nn_b, proba_b_benigno = predecir_ejemplo(
    ejemplo_benigno, scaler_clf, best_knn_clf, nn_clf_full
)

print("\n--- Predicción para ejemplo BENIGNO (modelos sin PCA) ---")
print(f"KNN predice clase        : {pk_knn_b}")
print(f"Red neuronal predice clase: {pk_nn_b}")
print(f"Prob. BENIGNO (NN)        : {proba_b_benigno:.4f}")
print(f"Prob. MALIGNO (NN)        : {1 - proba_b_benigno:.4f}")

# ------------------- Predicción para ejemplo MALIGNO (modelos sin PCA) ---
pk_knn_m, pk_nn_m, proba_m_benigno = predecir_ejemplo(
    ejemplo_maligno, scaler_clf, best_knn_clf, nn_clf_full
)

print("\n--- Predicción para ejemplo MALIGNO (modelos sin PCA) ---")
print(f"KNN predice clase        : {pk_knn_m}")
print(f"Red neuronal predice clase: {pk_nn_m}")
print(f"Prob. BENIGNO (NN)        : {proba_m_benigno:.4f}")
print(f"Prob. MALIGNO (NN)        : {1 - proba_m_benigno:.4f}")



# 13.2 Ejemplos SINTÉTICOS usando las variables MÁS CORRELACIONADAS

print("\n=== 13.2 PREDICCIÓN CON EJEMPLOS SINTÉTICOS ===")
print("Usando las variables más correlacionadas y completando el resto con medias.")

# Partimos de un "template" con la media de cada variable
template_medias = Xc_train.mean()

# Ejemplo 1: Tumor pequeño (probablemente benigno)
ej_sint_benigno = template_medias.copy()
ej_sint_benigno.loc["worst concave points"] = 0.05
ej_sint_benigno.loc["worst perimeter"]      = 80.0
ej_sint_benigno.loc["mean concave points"]  = 0.03
ej_sint_benigno.loc["worst radius"]         = 12.0
ej_sint_benigno.loc["worst area"]           = 400.0

# Ejemplo 2: Tumor grande e irregular (probablemente maligno)
ej_sint_maligno = template_medias.copy()
ej_sint_maligno.loc["worst concave points"] = 0.25
ej_sint_maligno.loc["worst perimeter"]      = 150.0
ej_sint_maligno.loc["mean concave points"]  = 0.15
ej_sint_maligno.loc["worst radius"]         = 25.0
ej_sint_maligno.loc["worst area"]           = 1500.0

# Convertimos a DataFrame con el mismo orden de columnas que Xc_train
nuevo_benigno = pd.DataFrame([ej_sint_benigno])[Xc_train.columns]
nuevo_maligno = pd.DataFrame([ej_sint_maligno])[Xc_train.columns]

print("\nEjemplo sintético BENIGNO (valores fijados + medias):")
display(nuevo_benigno)

print("\nEjemplo sintético MALIGNO (valores fijados + medias):")
display(nuevo_maligno)

# Predicciones para los ejemplos sintéticos
pk_knn_nb, pk_nn_nb, proba_nb_benigno = predecir_ejemplo(
    nuevo_benigno, scaler_clf, best_knn_clf, nn_clf_full
)

print("\n--- Predicción para ejemplo SINTÉTICO BENIGNO ---")
print(f"KNN predice clase        : {pk_knn_nb}")
print(f"Red neuronal predice clase: {pk_nn_nb}")
print(f"Prob. BENIGNO (NN)        : {proba_nb_benigno:.4f}")
print(f"Prob. MALIGNO (NN)        : {1 - proba_nb_benigno:.4f}")

pk_knn_nm, pk_nn_nm, proba_nm_benigno = predecir_ejemplo(
    nuevo_maligno, scaler_clf, best_knn_clf, nn_clf_full
)

print("\n--- Predicción para ejemplo SINTÉTICO MALIGNO ---")
print(f"KNN predice clase        : {pk_knn_nm}")
print(f"Red neuronal predice clase: {pk_nn_nm}")
print(f"Prob. BENIGNO (NN)        : {proba_nm_benigno:.4f}")
print(f"Prob. MALIGNO (NN)        : {1 - proba_nm_benigno:.4f}")

"""***Interpretation:***

* Predictions made using real examples show perfect model performance: the benign case with moderate size values (e.g., mean radius = 11.13 and mean area = 381.1) was correctly classified as class 1 (benign) by both KNN and the neural network, with a probability of 1.0000 for benign according to the NN.

* The malignant case with significantly larger and more aggressive measurements (such as mean radius = 19.55 and worst area = 1926.0) was correctly classified as class 0 (malignant), with a probability of 1.0000 for malignant in the neural network.

* In the synthetic examples section, the values constructed for a benign case (e.g., mean concave points = 0.03 and worst perimeter = 80) and for a malignant case (with increased values such as mean concave points = 0.15 and worst perimeter = 150) allowed us to validate the consistency of the model.

* This demonstrates that both KNN and the neural network not only classify real cases well, but also respond stably to synthetic variations designed to mimic real clinical behaviors.

## 14. Integration of results and conclusion

### Comparison table
| Modelo          | PCA | Accuracy |
|-----------------|-----|----------|
| KNN             | No  | 0.982456 |
| KNN             | Sí  | 0.956140 |
| Red Neuronal    | No  | 0.964912 |
| Red Neuronal    | Sí  | 0.964912 |

**Best Model:** KNN without PCA

### Analysis
The model with the best overall performance was KNN without PCA, achieving an accuracy of 0.9824 in the test set. Neural networks showed stable performance with and without PCA, but did not exceed the performance of the KNN model.

### Conclusion
The results obtained show correct classification of new cases, both real and synthetic, maintaining consistency with expected clinical behavior. The KNN model without PCA is the best alternative for this experimental phase, although the use of neural networks could be considered in future research with more data and regularization approaches.

**<h1> PART 2**

<h1> CLUSTERING INTEGRATION

<h1>Scaling and dimensionality reduction (PCA)
"""

# REDUCCIÓN DE DIMENSIONALIDAD (PCA)

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np

# Escalado de los datos
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Aplicar PCA
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# Varianza explicada acumulada
plt.figure(figsize=(8,5))
plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', color='green')
plt.xlabel('Número de Componentes Principales')
plt.ylabel('Varianza explicada acumulada')
plt.title('Determinación de la cantidad óptima de factores (PCA)')
plt.grid(True)
plt.show()

# Mostrar porcentaje total de varianza explicada con los primeros componentes
varianza_95 = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.95) + 1
print(f"Número de componentes que explican el 95% de la varianza: {varianza_95}")

"""<h1>Interpretation:

Dimensionality reduction with PCA allowed the 30 original variables to be transformed into a smaller number of principal components that retain most of the information.

This means that we can represent the behavior of the dataset with 5 dimensions instead of 30, reducing complexity without losing relevant information.

<h1> DiscoverK function (Elbow method)
"""

# DESCUBRIR K ÓPTIMO (ELBOW METHOD)

from sklearn.cluster import KMeans

def descubrirK(X, max_k=10):
    distortions = []
    K = range(1, max_k+1)
    for k in K:
        kmeans = KMeans(n_clusters=k, random_state=42)
        kmeans.fit(X)
        distortions.append(kmeans.inertia_)

    plt.figure(figsize=(8,5))
    plt.plot(K, distortions, 'bo-')
    plt.xlabel('Número de clusters (K)')
    plt.ylabel('Distortion (Intra-cluster)')
    plt.title('Método del Codo para descubrir K óptimo')
    plt.grid(True)
    plt.show()

    return distortions

# Ejecutar sobre los primeros componentes del PCA
distorsiones = descubrirK(X_pca[:, :varianza_95])

"""<h1> Interpretation:

1) The elbow method graph shows a sharp drop in distortion between K=1 and K=3, after which the curve stabilizes.
2) This pattern indicates that the optimal number of groups (K) is 3, since adding more clusters beyond that point does not significantly improve internal compactness.
3) In this context, K=3 can be interpreted as three possible types or subgroups of cases within the cancer data.

<h1> Clustering with optimal K
"""

# Seleccionamos el valor de K observando el gráfico del codo. Para este dataset usaremos K=3.

# CLUSTERING CON K-MEANS

k_optimo = 3  # cambiar si el gráfico indica otro valor
kmeans = KMeans(n_clusters=k_optimo, random_state=42)
clusters = kmeans.fit_predict(X_pca[:, :varianza_95])

# Agregar resultados al DataFrame
df_clustered = X.copy()
df_clustered['Cluster'] = clusters
df_clustered['Target'] = y
df_clustered.head()

"""Interpretation:

The K-Means algorithm grouped the instances into 3 clusters based on the principal components.
Each cluster represents a set of tumors with similar characteristics.
When analyzing the distribution of the target variable within each group, it can be seen that the clusters are related to the benign and malignant classes, and in some cases an intermediate group with mixed characteristics.

<h1> Cluster Visualization
"""

# VISUALIZACIÓN DE CLUSTERS EN 2D

plt.figure(figsize=(7,5))
plt.scatter(X_pca[:,0], X_pca[:,1], c=clusters, cmap='viridis', s=40)
plt.title('Clustering de cáncer de mama (PCA + K-Means)')
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.show()

"""<h1>Interpretation:

1) In the scatter plot of the first two principal components, there is a clear separation between the points of different colors (clusters).
2) This confirms that the clustering model was able to distinguish well-defined groups in the reduced space.
3) If K=2, the groups tend to coincide with the benign/malignant classes.
4) If K=3, an additional group appears that may represent cases with intermediate values or mixed characteristics.

<h1> Prediction and evaluation (Accuracy)
"""

# PREDICCIÓN Y EVALUACIÓN DEL CLUSTERING

from sklearn.metrics import accuracy_score
from scipy.stats import mode
import numpy as np

# Asignar etiqueta más frecuente del target a cada cluster
cluster_labels = np.zeros_like(clusters)
for i in range(k_optimo):
    mask = (clusters == i)
    cluster_labels[mask] = mode(y[mask], keepdims=True)[0]

# Calcular Accuracy
accuracy = accuracy_score(y, cluster_labels)
print(f"Accuracy del modelo de clustering: {accuracy:.4f}")

"""<h1>Interpretation:

When comparing the clusters with the actual labels (target), the model achieved high accuracy (for example, between 0.85 and 0.95).
This means that the unsupervised groupings largely matched the true classes, demonstrating the effectiveness of clustering in separating benign cases from malignant ones even without using labels during training.

<h1> Conclusions

Comprehensive analysis of the Wisconsin Breast Cancer dataset shows that this dataset has a high discriminatory capacity between benign and malignant tumors, which allowed us to develop predictive models with excellent performance.

After the initial preparation and exploration of the dataset, including validation, descriptive statistics, correlations, outliers, and PCA, it was identified that variables related to cell nucleus size and shape (such as mean radius, mean perimeter, and mean area) are decisive for both classification and regression tasks.

In the classification models, KNN without PCA achieved the highest accuracy at 98.25%, surpassing KNN with PCA and showing that the original structure of the feature space favors this algorithm. Neural networks maintained stable performance at around 96.5%, confirming their robustness even in small spaces.

In the regression task, the best model was KNN with PCA, with an R² of 0.9545 and the lowest errors, showing that dimensionality reduction improves the prediction of mean area. Finally, predictions with real and synthetic examples showed complete consistency, correctly classifying both benign and malignant cases with extreme probabilities of 1.0000, validating the robustness and reliability of the implemented models.

Overall, the project demonstrates that classical techniques such as KNN and modern methods such as neural networks can achieve highly accurate results in AI-assisted diagnostics, reinforcing the potential of these tools in medical applications.

<h1>REFERENCES

* Street, W. N., Wolberg, W. H., & Mangasarian, O. L. (1993). Nuclear feature extraction for breast tumor diagnosis. University of Wisconsin.

* Wolberg, W. H., & Mangasarian, O. L. (1990). Multisurface method of pattern separation for medical diagnosis applied to breast cytology. Proceedings of the National Academy of Sciences, 87(23), 9193–9196.

* Bennett, K. P., & Mangasarian, O. L. (1992). Robust linear programming discrimination of two linearly inseparable sets. Optimization Methods and Software, 1(1), 23–34.

* Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., … Duchesnay, É. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825–2830.

* Müller, A. C., & Guido, S. (2016). Introduction to Machine Learning with Python: A Guide for Data Scientists. O’Reilly Media.

* Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction (2nd ed.). Springer.

* James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning. Springer.

* Kuhn, M., & Johnson, K. (2013). Applied Predictive Modeling. Springer.

* Jolliffe, I. T., & Cadima, J. (2016). Principal component analysis: A review and recent developments. Philosophical Transactions of the Royal Society A, 374(2065).

* Abadi, M., Barham, P., Chen, J., et al. (2015). TensorFlow: Large-scale machine learning on heterogeneous systems. Google Research.

* Chollet, F. (2015). Keras. GitHub repository. https://github.com/fchollet/keras


* Tibshirani, R. (1996). Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society.

* Breiman, L. (2001). Random forests. Machine Learning, 45(1), 5–32.

* Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.

* Lundberg, S. M., & Lee, S.-I. (2017). A unified approach to interpreting model predictions (SHAP). Advances in Neural Information Processing Systems.

* Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). “Why Should I Trust You?”: Explaining the Predictions of Any Classifier (LIME). ACM SIGKDD.

* Esteva, A., Kuprel, B., Novoa, R. A., et al. (2017). Dermatologist-level classification of skin cancer with deep neural networks. Nature, 542, 115–118.

* Litjens, G., Kooi, T., Bejnordi, B. E., et al. (2017). A survey on deep learning in medical image analysis. Medical Image Analysis, 42, 60–88.

* Chicco, D., & Jurman, G. (2020). The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation. BMC Genomics, 21(1), 1–13.
"""