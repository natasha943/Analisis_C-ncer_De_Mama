# -*- coding: utf-8 -*-
"""Analisis_Cancer_De_Mama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16DQGbx7ekC5kschn1Z74G2DYkYk-JS8e

<h1> DATASET BREAST CANCER WISCONSIN

DESCRIPCION:

1. CARGA DEL DATASET
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import load_breast_cancer

# Cargar dataset
data = load_breast_cancer(as_frame=True)
df = data.frame

print("Shape:", df.shape)
df.head()

"""<h1> 2. RESUMEN ESTADISTICO"""

df.describe().T

"""<h1>3. INFORMACION DEL DATASET"""

print("\n=== 3. INFORMACIÓN DEL DATASET (df.info) ===")
df.info()

"""<h1>4. VALORES FALTANTES"""

print("\n=== 4. ANÁLISIS DE VALORES FALTANTES ===")
missing = df.isna().sum()
display(missing)

print("\nTotal de valores faltantes en todo el dataset:", int(missing.sum()))

"""<h1> 5. Distribucion de la variable objetivo"""

# Top 10 variables más correlacionadas con la clase (excluyendo 'target' de la lista)
top_corr = df.corr()['target'].abs().sort_values(ascending=False).head(11)
print("Top 10 variables más correlacionadas con 'target':")
print(top_corr.to_string())

# Gráfico de barras horizontales
plt.figure(figsize=(8, 6))
# Excluimos 'target' que siempre tendría correlación 1.0 consigo misma
top_corr_features = top_corr.drop('target') if 'target' in top_corr.index else top_corr
bars = plt.barh(top_corr_features.index, top_corr_features.values, color='salmon')
plt.title("Top 10 variables más correlacionadas con el diagnóstico (target)", fontsize=14)
plt.xlabel("Correlación absoluta", fontsize=12)
plt.ylabel("Variable", fontsize=12)
plt.gca().invert_yaxis()  # Para que la variable más correlacionada quede arriba
plt.grid(axis='x', alpha=0.3)

# Añadir valores en las barras
for bar in bars:
    width = bar.get_width()
    plt.text(width + 0.01, bar.get_y() + bar.get_height()/2,
             f'{width:.3f}', ha='left', va='center', fontsize=10)

plt.tight_layout()
plt.show()

# Seleccionar las características más relevantes (excluyendo 'target')
selected_features = top_corr.drop('target').index.tolist() if 'target' in top_corr.index else top_corr.index.tolist()
print(f"\nCaracterísticas seleccionadas ({len(selected_features)}): {selected_features}")

# Crear dataset reducido (incluyendo 'target' para análisis posterior)
df_reduced = df[selected_features + ['target']].copy()
print(f"\nDataset reducido - Dimensiones: {df_reduced.shape}")
print(f"Variables: {df_reduced.columns.tolist()}")

# Mostrar primeras filas del dataset reducido
print("\nPrimeras 5 filas del dataset reducido:")
display(df_reduced.head())

# Análisis adicional: correlación entre las características seleccionadas
print("\n=== ANÁLISIS ADICIONAL DE CARACTERÍSTICAS SELECCIONADAS ===")

# Matriz de correlación entre características seleccionadas
plt.figure(figsize=(10, 8))
corr_matrix_reduced = df_reduced.corr()
sns.heatmap(corr_matrix_reduced, annot=True, fmt=".2f", cmap="coolwarm",
            center=0, square=True, linewidths=0.5, cbar_kws={"shrink": 0.8})
plt.title("Matriz de correlación - Características seleccionadas", fontsize=14)
plt.tight_layout()
plt.show()

# Verificar multicolinealidad (correlaciones altas entre predictores)
print("\nCorrelaciones altas (>0.8) entre predictores seleccionados:")
high_corr_pairs = []
for i in range(len(selected_features)):
    for j in range(i+1, len(selected_features)):
        feat1, feat2 = selected_features[i], selected_features[j]
        corr_value = abs(df_reduced[feat1].corr(df_reduced[feat2]))
        if corr_value > 0.8:
            high_corr_pairs.append((feat1, feat2, corr_value))

if high_corr_pairs:
    for feat1, feat2, corr in high_corr_pairs:
        print(f"  {feat1} - {feat2}: {corr:.3f}")
else:
    print("  No hay correlaciones extremadamente altas (>0.8) entre predictores seleccionados.")

# Estadísticas descriptivas de las características seleccionadas
print("\nResumen estadístico de características seleccionadas:")
display(df_reduced[selected_features].describe().T)

# Distribución de las características seleccionadas por clase
print("\nValores promedio por clase para características seleccionadas:")
for feature in selected_features[:5]:  # Mostrar solo las primeras 5 para brevedad
    mean_by_class = df_reduced.groupby('target')[feature].mean()
    print(f"\n{feature}:")
    print(f"  Clase 0 (Maligno): {mean_by_class[0]:.3f}")
    print(f"  Clase 1 (Benigno): {mean_by_class[1]:.3f}")
    print(f"  Diferencia: {abs(mean_by_class[0] - mean_by_class[1]):.3f}")

# Guardar dataset reducido para uso posterior (opcional)
df_reduced.to_csv("breast_cancer_reduced_features.csv", index=False)
print(f"\nDataset reducido guardado como 'breast_cancer_reduced_features.csv'")

"""<h1> Interpretacion <br>

* Las variables relacionadas con tamaño (radius, area, perimeter) y forma (concavity, concave points) son las más importantes para distinguir entre tumores.

* El dataset reducido conserva las características más discriminativas
Se seleccionaron las 10 variables con mayor correlación + target, obteniendo:

  11 columnas finales

  569 filas (sin pérdida de muestras)

* Las características seleccionadas están altamente correlacionadas entre sí
Se observaron correlaciones extremadamente altas entre predictores.

* Las diferencias estadísticas entre clases son muy grandes: los valores promedio entre maligno (0) y benigno (1) muestran diferencias significativas.

* Las correlaciones entre predictores son muy altas, mostrando redundancia estructural útil para PCA.

<h1> 6. MATRIZ DE CORRELACION
"""

print("\n=== 6. MATRIZ DE CORRELACIÓN ENTRE VARIABLES ===")
plt.figure(figsize=(12,10))
sns.heatmap(df.corr(), cmap="coolwarm")
plt.title("Matriz de correlación")
plt.show()

"""<h1> Interpretacion <br>

La matriz de correlación evidencia una alta multicolinealidad entre varias características del dataset, especialmente entre las variables morfológicas relacionadas con el tamaño y forma del tumor, como radius, perimeter, area y sus versiones mean, worst y error. Estas variables muestran correlaciones superiores al 0.8, lo cual indica que aportan información redundante.

<h1> 7. VISUALIZACION DE HISTOGRAMAS / VARIABLES NUMÈRICAS
"""

print("\n=== 7. HISTOGRAMAS DE LAS VARIABLES NUMÉRICAS ===")

df.drop(columns=['target']).hist(bins=30, figsize=(16,14))
plt.suptitle("Distribución de variables numéricas", y=1.02)
plt.show()

"""<h1> Interpretacion <br>

Los histogramas muestran que la mayoría de las variables numéricas del dataset presentan distribuciones asimétricas (sesgo hacia la derecha), especialmente las relacionadas con errores (“error”) y valores “worst”. Esto indica que existen muchos casos con valores pequeños y pocos con valores muy grandes.

 Las variables principales como mean radius, mean perimeter y mean area presentan formas más cercanas a una distribución normal, pero aún con ligeros sesgos. En conjunto, los histogramas confirman la presencia de escalas muy diferentes y variabilidad considerable

<h1> 8. Boxplots para outliers
"""

print("\n=== 8. BOXPLOTS Y ANÁLISIS DE OUTLIERS ===")

plt.figure(figsize=(14, 8))
sns.boxplot(data=df.drop(columns=['target']))
plt.xticks(rotation=90)
plt.title("Boxplots por variable (sin incluir 'target')", fontsize=14)
plt.xlabel("Variables", fontsize=12)
plt.ylabel("Valores", fontsize=12)
plt.tight_layout()
plt.show()


df_numeric = df.drop(columns=['target'])

# Calcular límites usando IQR
Q1 = df_numeric.quantile(0.25)
Q3 = df_numeric.quantile(0.75)
IQR = Q3 - Q1

lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

print("\n" + "="*60)
print("ANÁLISIS DETALLADO DE OUTLIERS")
print("="*60)

# Contar outliers por variable
outliers_count = {}
outliers_percentage = {}

for col in df_numeric.columns:
    lower_col = lower_bound[col]
    upper_col = upper_bound[col]

    outliers_mask = (df_numeric[col] < lower_col) | (df_numeric[col] > upper_col)
    outliers_count[col] = outliers_mask.sum()
    outliers_percentage[col] = (outliers_mask.sum() / len(df_numeric)) * 100

# Crear DataFrame con resultados
outliers_df = pd.DataFrame({
    'Variable': list(outliers_count.keys()),
    'N_Outliers': list(outliers_count.values()),
    '%_Outliers': list(outliers_percentage.values())
})

print("\nTop 10 variables con más outliers:")
outliers_df_sorted = outliers_df.sort_values('N_Outliers', ascending=False)
display(outliers_df_sorted.head(10))

# Análisis general
total_outliers_detected = outliers_df['N_Outliers'].sum()
rows_with_any_outlier = ((df_numeric < lower_bound) | (df_numeric > upper_bound)).any(axis=1)
rows_affected = rows_with_any_outlier.sum()

print(f"\nEstadísticas generales:")
print(f"- Total outliers detectados: {total_outliers_detected}")
print(f"- Filas con al menos 1 outlier: {rows_affected} ({(rows_affected/len(df))*100:.1f}%)")

# Guardar dataset original para Fase 2
df.to_csv("breast_cancer_original.csv", index=False)
print("\n✓ Dataset guardado para Fase 2: 'breast_cancer_original.csv'")

"""<h1> Interpretacion <br>

* El análisis mediante el método IQR (Interquartile Range) permitió identificar valores extremadamente alejados de la distribución central del dataset WDBC.
* El dataset contiene una cantidad considerable de outliers (608), lo que representa un 30.1% de las filas con al menos un valor extremo.

* Las variables más afectadas son las relacionadas con errores y medidas de tamaño, lo cual es coherente con su naturaleza clínica.

<h1> 9. ANÁLISIS POR CLASE (MALIGNO VS BENIGNO)
"""

print("\n" + "="*70)
print("ANÁLISIS POR CLASE: MALIGNO (0) vs BENIGNO (1)")
print("="*70)

# =============================================
# 1. SELECCIÓN DE VARIABLES CLAVE MÁS INFORMATIVAS
# =============================================
print("\n1. SELECCIÓN DE VARIABLES PARA ANÁLISIS")

# Basado en análisis de correlación anterior, seleccionamos las más relevantes
features_to_plot = [
    'mean radius',           # Tamaño del tumor
    'mean texture',          # Textura del tejido
    'mean perimeter',        # Perímetro del tumor
    'mean area',             # Área del tumor
    'mean concavity',        # Concavidad (indicador de malignidad)
    'mean concave points'    # Puntos cóncavos (importante según correlación)
]

print(f"   Analizando {len(features_to_plot)} variables clave:")
for i, feature in enumerate(features_to_plot, 1):
    print(f"   {i}. {feature}")

# =============================================
# 2. ANÁLISIS ESTADÍSTICO POR CLASE
# =============================================
print("\n2. ESTADÍSTICAS DESCRIPTIVAS POR CLASE")
print("   " + "-"*50)

# Crear DataFrame con estadísticas comparativas
stats_comparison = []

for feature in features_to_plot:
    if feature in df.columns:
        # Estadísticas para clase 0 (Maligno)
        stats_0 = df[df['target'] == 0][feature].describe()
        # Estadísticas para clase 1 (Benigno)
        stats_1 = df[df['target'] == 1][feature].describe()

        stats_comparison.append({
            'Variable': feature,
            'Maligno_Media': stats_0['mean'],
            'Maligno_Std': stats_0['std'],
            'Benigno_Media': stats_1['mean'],
            'Benigno_Std': stats_1['std'],
            'Diferencia_Media': stats_0['mean'] - stats_1['mean'],
            'Diferencia_%': ((stats_0['mean'] - stats_1['mean']) / stats_1['mean']) * 100
        })

# Convertir a DataFrame y mostrar
stats_df = pd.DataFrame(stats_comparison)
print("\n   Comparación de medias por clase:")
display(stats_df[['Variable', 'Maligno_Media', 'Benigno_Media', 'Diferencia_Media', 'Diferencia_%']])

# =============================================
# 3. VISUALIZACIÓN MEJORADA: SUBPLOTS ORGANIZADOS
# =============================================
print("\n3. VISUALIZACIÓN COMPARATIVA")

# Crear figura con subplots organizados
fig = plt.figure(figsize=(16, 12))

# Título principal
plt.suptitle('ANÁLISIS POR CLASE: MALIGNO vs BENIGNO', fontsize=16, y=1.02)

# BOXPLOTS (fila superior)
print("\n   Boxplots por clase (diferencias de distribución):")
for idx, feature in enumerate(features_to_plot[:3], 1):  # Primeras 3
    ax = plt.subplot(3, 3, idx)

    # Crear boxplot con mejor estilo
    boxprops = dict(linestyle='-', linewidth=1.5)
    medianprops = dict(linestyle='-', linewidth=2.5, color='red')
    whiskerprops = dict(linestyle='--', linewidth=1.5)

    bp = ax.boxplot([
        df[df['target'] == 0][feature].dropna(),
        df[df['target'] == 1][feature].dropna()
    ],
    labels=['Maligno (0)', 'Benigno (1)'],
    patch_artist=True,
    boxprops=boxprops,
    medianprops=medianprops,
    whiskerprops=whiskerprops)

    # Colorear las cajas
    colors = ['#FF6B6B', '#4ECDC4']  # Rojo para maligno, verde agua para benigno
    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)

    ax.set_title(feature, fontsize=12, fontweight='bold')
    ax.set_ylabel('Valor', fontsize=10)
    ax.grid(True, alpha=0.3, axis='y')

    # Añadir anotación con diferencia de medias
    mean_diff = stats_df.loc[stats_df['Variable'] == feature, 'Diferencia_Media'].values[0]
    ax.text(0.5, 0.95, f'Δ = {mean_diff:.2f}',
            transform=ax.transAxes, ha='center', va='top',
            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))

# HISTOGRAMAS SUPERPUESTOS (fila media)
print("\n   Histogramas superpuestos (distribuciones):")
for idx, feature in enumerate(features_to_plot[3:6], 4):  # Siguientes 3
    ax = plt.subplot(3, 3, idx)

    # Histograma para clase 0 (Maligno)
    ax.hist(df[df['target'] == 0][feature].dropna(),
            bins=30, alpha=0.6, color='#FF6B6B',
            label='Maligno (0)', density=True)

    # Histograma para clase 1 (Benigno)
    ax.hist(df[df['target'] == 1][feature].dropna(),
            bins=30, alpha=0.6, color='#4ECDC4',
            label='Benigno (1)', density=True)

    ax.set_title(feature, fontsize=12, fontweight='bold')
    ax.set_xlabel('Valor', fontsize=10)
    ax.set_ylabel('Densidad', fontsize=10)
    ax.legend(fontsize=9)
    ax.grid(True, alpha=0.3)

    # Añadir líneas de media
    mean_0 = df[df['target'] == 0][feature].mean()
    mean_1 = df[df['target'] == 1][feature].mean()

    ax.axvline(mean_0, color='#FF6B6B', linestyle='--', linewidth=2, alpha=0.8)
    ax.axvline(mean_1, color='#4ECDC4', linestyle='--', linewidth=2, alpha=0.8)

# VIOLIN PLOTS (fila inferior)
print("\n   Violin plots (distribución + densidad):")
for idx, feature in enumerate(features_to_plot[3:6], 7):  # Últimas 3 (usando diferentes)
    ax = plt.subplot(3, 3, idx)

    # Crear violin plot
    violin_data = [df[df['target'] == 0][feature].dropna(),
                   df[df['target'] == 1][feature].dropna()]

    vp = ax.violinplot(violin_data, showmeans=True, showmedians=True)

    # Personalizar violines
    colors = ['#FF6B6B', '#4ECDC4']
    for i, pc in enumerate(vp['bodies']):
        pc.set_facecolor(colors[i])
        pc.set_alpha(0.7)
        pc.set_edgecolor('black')

    # Personalizar estadísticas
    vp['cmeans'].set_color('yellow')
    vp['cmedians'].set_color('black')
    vp['cbars'].set_color('black')
    vp['cmins'].set_color('black')
    vp['cmaxes'].set_color('black')

    ax.set_xticks([1, 2])
    ax.set_xticklabels(['Maligno', 'Benigno'], fontsize=10)
    ax.set_title(feature, fontsize=12, fontweight='bold')
    ax.set_ylabel('Valor', fontsize=10)
    ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

# =============================================
# 4. PRUEBAS ESTADÍSTICAS DE DIFERENCIAS
# =============================================
print("\n4. PRUEBAS ESTADÍSTICAS DE SIGNIFICANCIA")
print("   " + "-"*50)

from scipy import stats

print("\n   Prueba t-Student para diferencias de medias:")
print("   " + "-"*50)
print(f"{'Variable':<25} {'t-statistic':<15} {'p-value':<15} {'Significativo (p<0.05)':<20}")
print("   " + "-"*50)

for feature in features_to_plot:
    if feature in df.columns:
        # Datos por clase
        data_0 = df[df['target'] == 0][feature].dropna()
        data_1 = df[df['target'] == 1][feature].dropna()

        # Prueba t de Student (asumiendo varianzas diferentes)
        t_stat, p_value = stats.ttest_ind(data_0, data_1, equal_var=False)

        # Determinar significancia
        significativo = "SÍ" if p_value < 0.05 else "NO"

        print(f"{feature:<25} {t_stat:>10.3f}    {p_value:>12.4f}    {significativo:^20}")

# =============================================
# 5. ANÁLISIS DE SOLAPAMIENTO (OVERLAP)
# =============================================
print("\n5. ANÁLISIS DE SOLAPAMIENTO ENTRE CLASES")
print("   " + "-"*50)

print("\n   Variables con menor solapamiento (mejores para clasificación):")

overlap_analysis = []
for feature in features_to_plot:
    if feature in df.columns:
        # Calcular rango de valores
        range_0 = df[df['target'] == 0][feature].agg(['min', 'max'])
        range_1 = df[df['target'] == 1][feature].agg(['min', 'max'])

        # Calcular solapamiento
        overlap_min = max(range_0['min'], range_1['min'])
        overlap_max = min(range_0['max'], range_1['max'])

        if overlap_max > overlap_min:
            overlap_range = overlap_max - overlap_min
            total_range = max(range_0['max'], range_1['max']) - min(range_0['min'], range_1['min'])
            overlap_percentage = (overlap_range / total_range) * 100
        else:
            overlap_percentage = 0

        overlap_analysis.append({
            'Variable': feature,
            'Solapamiento_%': overlap_percentage,
            'Rango_Maligno': f"{range_0['min']:.1f}-{range_0['max']:.1f}",
            'Rango_Benigno': f"{range_1['min']:.1f}-{range_1['max']:.1f}"
        })

# Ordenar por menor solapamiento (mejor para clasificación)
overlap_df = pd.DataFrame(overlap_analysis).sort_values('Solapamiento_%')
display(overlap_df)

# =============================================
# 6. VISUALIZACIÓN DE SEPARACIÓN IDEAL
# =============================================
print("\n6. VISUALIZACIÓN: MEJORES VARIABLES PARA CLASIFICACIÓN")

# Seleccionar las 2 variables con menor solapamiento
best_features = overlap_df.head(2)['Variable'].tolist()

if len(best_features) >= 2:
    fig, ax = plt.subplots(1, 2, figsize=(14, 5))

    for idx, feature in enumerate(best_features):
        # Scatter plot con más transparencia para ver densidad
        ax[idx].scatter(df[df['target'] == 0][feature],
                       np.random.normal(0, 0.05, size=len(df[df['target'] == 0])),
                       alpha=0.6, color='#FF6B6B', s=30, label='Maligno')

        ax[idx].scatter(df[df['target'] == 1][feature],
                       np.random.normal(1, 0.05, size=len(df[df['target'] == 1])),
                       alpha=0.6, color='#4ECDC4', s=30, label='Benigno')

        ax[idx].set_xlabel(feature, fontsize=11)
        ax[idx].set_yticks([0, 1])
        ax[idx].set_yticklabels(['Maligno', 'Benigno'], fontsize=10)
        ax[idx].set_title(f'{feature}\n(Solapamiento: {overlap_df.loc[overlap_df["Variable"]==feature, "Solapamiento_%"].values[0]:.1f}%)',
                         fontsize=12, fontweight='bold')
        ax[idx].legend(fontsize=9)
        ax[idx].grid(True, alpha=0.3)

    plt.suptitle('Variables con MEJOR Separación entre Clases', fontsize=14, y=1.05)
    plt.tight_layout()
    plt.show()

"""<h1> CONCLUSIONES

Los tumores malignos presentan valores significativamente mayores en mean radius, mean texture, mean perimeter, mean area, mean concavity y mean concave points.

Las diferencias (Δ) muestran una separación clara entre ambas clases, especialmente en medidas de tamaño y forma.

Los histogramas y violin plots evidencian que las distribuciones de ambas clases se superponen poco, lo que indica que estas variables son altamente útiles para distinguir entre tumores benignos y malignos.

En conjunto, la gráfica confirma que las características morfológicas del tejido mamario contienen patrones muy marcados que permiten diferenciar con alta precisión ambas clases.

<h1> 10. GUARDAR EL DATASET LIMPIO
"""

clean = df.copy()
clean.to_csv("Examen_Listo_Fase1.csv", index=False)
print("Dataset limpio guardado como Examen_Listo_Fase1.csv")

"""# Sección nueva

<h1> FASE 2 — MODELOS, OPTIMIZACIÓN Y EVALUACIÓN

<h1> Importar Librerias
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report,
    mean_absolute_error, mean_squared_error, r2_score
)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Dense, Dropout, Input

"""<h1> 1. Cargar dataset limpio"""

import pandas as pd
df = pd.read_csv("Examen_Listo_Fase1.csv")
X = df.drop(columns=['target'])
y = df['target']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
   X, y, test_size=0.20, random_state=42, stratify=y)
print("Dataset Listo")

"""<h1> 2. Split Train–Test y Escalado"""

from sklearn.preprocessing import StandardScaler

print("\n=== 2. CLASIFICACIÓN – PREPARACIÓN Y ESCALADO ===")

# Usamos el MISMO split global definido antes
Xc_train = X_train.copy()
Xc_test  = X_test.copy()
yc_train = y_train.copy()
yc_test  = y_test.copy()

print("Xc_train:", Xc_train.shape, "Xc_test:", Xc_test.shape)

# Estandarización
scaler_clf = StandardScaler()
Xc_train_scaled = scaler_clf.fit_transform(Xc_train)
Xc_test_scaled  = scaler_clf.transform(Xc_test)

"""***Interpretacion:***

La división del dataset produjo 455 muestras para entrenamiento y 114 para prueba, manteniendo las 30 variables predictoras en ambos conjuntos. Esta separación permite entrenar el modelo sin sesgos y evaluar su capacidad de generalización de manera objetiva.

Esto indica que se aplicó correctamente un train-test split típico del 80%–20%:

80% para entrenamiento → 455 registros

20% para prueba → 114 registros

Esto evita sobreajuste (overfitting) y permite medir la capacidad real del modelo para generalizar.

<h1> 3. Preprocesamiento
"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train)
X_test_s = scaler.transform(X_test)

"""<h1> 4. Modelos baseline"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score

logreg = LogisticRegression(max_iter=500)
logreg.fit(X_train_s, y_train)

pred = logreg.predict(X_test_s)
probs = logreg.predict_proba(X_test_s)[:,1]

print("Accuracy:", accuracy_score(y_test, pred))
print("F1:", f1_score(y_test, pred))
print("AUC:", roc_auc_score(y_test, probs))

"""<h1> 5. PCA PARA CLASIFICACIÓN (REDUCCIÓN DIMENSION)"""

from sklearn.decomposition import PCA
import numpy as np
import matplotlib.pyplot as plt

print("\n=== 3. PCA PARA CLASIFICACIÓN ===")

# PCA completo para estudiar varianza
pca_tmp = PCA(n_components=None, random_state=42)
pca_tmp.fit(Xc_train_scaled)

var_exp = np.cumsum(pca_tmp.explained_variance_ratio_)
n_comp_95 = np.argmax(var_exp >= 0.95) + 1
print(f"Número de componentes para explicar ~95% varianza: {n_comp_95}")

plt.figure(figsize=(6,4))
plt.plot(var_exp, marker="o")
plt.axhline(0.95, linestyle="--")
plt.xlabel("Número de componentes")
plt.ylabel("Varianza explicada acumulada")
plt.title("PCA – Varianza acumulada (clasificación)")
plt.grid(True)
plt.show()

# PCA definitivo
pca_clf = PCA(n_components=n_comp_95, random_state=42)
Xc_train_pca = pca_clf.fit_transform(Xc_train_scaled)
Xc_test_pca  = pca_clf.transform(Xc_test_scaled)

print("Xc_train_pca:", Xc_train_pca.shape, "Xc_test_pca:", Xc_test_pca.shape)

"""El PCA es altamente efectivo en este dataset. Con solo 10 componentes se conserva la mayor parte de la información relevante (95%), reduciendo significativamente la dimensionalidad frente a las 30 variables originales. Esto permite mejorar la eficiencia computacional y disminuir redundancia entre características altamente correlacionadas.

<h1> 6. CLASIFICACIÓN – MODELO 1: KNN (ORIGINAL + PCA)
"""

print("\n=== 6. CLASIFICACIÓN – MODELO 1: KNN ===")

param_grid_knn = {
    "n_neighbors": [3,5,7,9,11],
    "weights": ["uniform", "distance"],
    "metric": ["euclidean", "manhattan"]
}

# ---- KNN sobre datos ESCALADOS (sin PCA) ----
knn_clf = KNeighborsClassifier()
grid_knn_clf = GridSearchCV(
    knn_clf, param_grid_knn,
    cv=5, scoring="accuracy", n_jobs=-1
)
grid_knn_clf.fit(Xc_train_scaled, yc_train)

print("Mejores parámetros KNN (sin PCA):", grid_knn_clf.best_params_)
best_knn_clf = grid_knn_clf.best_estimator_

yc_pred_knn = best_knn_clf.predict(Xc_test_scaled)
acc_knn = accuracy_score(yc_test, yc_pred_knn)
print("Accuracy KNN (sin PCA):", acc_knn)

# ---- KNN sobre datos PCA ----
knn_clf_pca = KNeighborsClassifier()
grid_knn_clf_pca = GridSearchCV(
    knn_clf_pca, param_grid_knn,
    cv=5, scoring="accuracy", n_jobs=-1
)
grid_knn_clf_pca.fit(Xc_train_pca, yc_train)

print("Mejores parámetros KNN (con PCA):", grid_knn_clf_pca.best_params_)
best_knn_clf_pca = grid_knn_clf_pca.best_estimator_

yc_pred_knn_pca = best_knn_clf_pca.predict(Xc_test_pca)
acc_knn_pca = accuracy_score(yc_test, yc_pred_knn_pca)
print("Accuracy KNN (con PCA):", acc_knn_pca)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

print("\n=== 6. CLASIFICACIÓN – MODELO 1: KNN ===")

param_grid_knn = {
    "n_neighbors": [3,5,7,9,11],
    "weights": ["uniform", "distance"],
    "metric": ["euclidean", "manhattan"]
}

# ============================================================
# 6.1 KNN SIN PCA (datos escalados)
# ============================================================
knn_clf = KNeighborsClassifier()
grid_knn_clf = GridSearchCV(
    knn_clf, param_grid_knn,
    cv=5, scoring="accuracy", n_jobs=-1
)
grid_knn_clf.fit(Xc_train_scaled, yc_train)

best_knn_clf = grid_knn_clf.best_estimator_
yc_pred_knn = best_knn_clf.predict(Xc_test_scaled)

# Métricas sin PCA
acc_knn  = accuracy_score(yc_test, yc_pred_knn)
prec_knn = precision_score(yc_test, yc_pred_knn)
rec_knn  = recall_score(yc_test, yc_pred_knn)
f1_knn   = f1_score(yc_test, yc_pred_knn)

cm_knn = confusion_matrix(yc_test, yc_pred_knn)

# ============================================================
# 6.2 KNN CON PCA
# ============================================================
knn_clf_pca = KNeighborsClassifier()
grid_knn_clf_pca = GridSearchCV(
    knn_clf_pca, param_grid_knn,
    cv=5, scoring="accuracy", n_jobs=-1
)
grid_knn_clf_pca.fit(Xc_train_pca, yc_train)

best_knn_clf_pca = grid_knn_clf_pca.best_estimator_
yc_pred_knn_pca = best_knn_clf_pca.predict(Xc_test_pca)

# Métricas con PCA
acc_knn_pca  = accuracy_score(yc_test, yc_pred_knn_pca)
prec_knn_pca = precision_score(yc_test, yc_pred_knn_pca)
rec_knn_pca  = recall_score(yc_test, yc_pred_knn_pca)
f1_knn_pca   = f1_score(yc_test, yc_pred_knn_pca)

cm_knn_pca = confusion_matrix(yc_test, yc_pred_knn_pca)

# ============================================================
# 6.3 TABLA AUTOMÁTICA DE RESULTADOS
# ============================================================
tabla_knn = pd.DataFrame({
    "Modelo": ["KNN (sin PCA)", "KNN (con PCA)"],
    "Accuracy": [acc_knn, acc_knn_pca],
    "Precision": [prec_knn, prec_knn_pca],
    "Recall": [rec_knn, rec_knn_pca],
    "F1-Score": [f1_knn, f1_knn_pca],
    "Mejores Parámetros": [
        str(grid_knn_clf.best_params_),
        str(grid_knn_clf_pca.best_params_)
    ]
})

print("\n=== TABLA DE MÉTRICAS KNN ===")
display(tabla_knn)

# ============================================================
# 6.4 MATRICES DE CONFUSIÓN VISUALES
# ============================================================
plt.figure(figsize=(12,5))

# ------------------ KNN SIN PCA ------------------
plt.subplot(1,2,1)
sns.heatmap(cm_knn, annot=True, fmt="d", cmap="Blues")
plt.title("KNN – Sin PCA")
plt.xlabel("Predicción")
plt.ylabel("Real")

# ------------------ KNN CON PCA ------------------
plt.subplot(1,2,2)
sns.heatmap(cm_knn_pca, annot=True, fmt="d", cmap="Oranges")
plt.title("KNN – Con PCA")
plt.xlabel("Predicción")
plt.ylabel("Real")

plt.tight_layout()
plt.show()

"""<h1>Conclusion<br>

La matriz de confusión muestra que el modelo KNN obtiene un rendimiento muy similar con y sin PCA. Sin PCA, el modelo clasifica correctamente 112 de 114 casos, mientras que con PCA acierta 109 de 114. Esto indica que la reducción de dimensionalidad mantiene prácticamente la misma capacidad de clasificación, aunque introduce una ligera pérdida de precisión. Aun así, el PCA sigue siendo una opción válida cuando se busca reducir complejidad o mejorar eficiencia computacional sin sacrificar significativamente el desempeño.

<h1> 7. CLASIFICACIÓN – MODELO 2: RED NEURONAL (KERAS)

---
"""

print("\n=== 7. CLASIFICACIÓN – MODELO 2: RED NEURONAL (KERAS) ===")

def build_nn_classifier(input_dim, n_hidden1=64, n_hidden2=32, lr=0.001, dropout_rate=0.2):
    model = Sequential()
    model.add(Input(shape=(input_dim,)))
    model.add(Dense(n_hidden1, activation="relu"))
    model.add(Dropout(dropout_rate))
    model.add(Dense(n_hidden2, activation="relu"))
    model.add(Dropout(dropout_rate))
    model.add(Dense(1, activation="sigmoid"))
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
    model.compile(
        loss="binary_crossentropy",
        optimizer=optimizer,
        metrics=["accuracy"]
    )
    return model

early_stop = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)

# ---------- Entrenamiento NN en datos ESCALADOS ----------
input_dim_full = Xc_train_scaled.shape[1]
nn_clf_full = build_nn_classifier(input_dim=input_dim_full, n_hidden1=64, n_hidden2=32, lr=0.001)

history_full = nn_clf_full.fit(
    Xc_train_scaled, yc_train,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[early_stop],
    verbose=0
)

yc_pred_proba_full = nn_clf_full.predict(Xc_test_scaled).ravel()
yc_pred_nn_full = (yc_pred_proba_full >= 0.5).astype(int)

acc_nn_full = accuracy_score(yc_test, yc_pred_nn_full)
print("Accuracy NN (sin PCA):", acc_nn_full)

# ---------- Entrenamiento NN en datos PCA ----------
input_dim_pca = Xc_train_pca.shape[1]
nn_clf_pca = build_nn_classifier(input_dim=input_dim_pca, n_hidden1=32, n_hidden2=16, lr=0.001)

history_pca = nn_clf_pca.fit(
    Xc_train_pca, yc_train,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[early_stop],
    verbose=0
)

yc_pred_proba_pca = nn_clf_pca.predict(Xc_test_pca).ravel()
yc_pred_nn_pca = (yc_pred_proba_pca >= 0.5).astype(int)

acc_nn_pca = accuracy_score(yc_test, yc_pred_nn_pca)
print("Accuracy NN (con PCA):", acc_nn_pca)

"""<h1> 8. REGRESIÓN – PREPARACIÓN"""

print("\n=== 8. REGRESIÓN – PREPARACIÓN ===")

# Elegimos una variable continua como objetivo de regresión
target_reg = "mean area"
y_reg = df[target_reg]
X_reg = df.drop(columns=["target", target_reg])

Xr_train, Xr_test, yr_train, yr_test = train_test_split(
    X_reg, y_reg, test_size=0.2, random_state=42
)

print("Xr_train:", Xr_train.shape, "Xr_test:", Xr_test.shape)

# -------------------------------------------------------
# 8.1 Descripción básica de la variable de regresión
# -------------------------------------------------------
print("\nDescripción estadística de la variable objetivo (mean area):")
display(yr_train.describe().to_frame(name=target_reg))

# Correlación de cada variable con la variable objetivo
corr_reg = df.drop(columns=["target"]).corr()[target_reg].sort_values(ascending=False)
top_corr_reg = corr_reg.head(10).to_frame(name="Correlación con 'mean area'")
print("\nTop 10 variables más correlacionadas con la variable objetivo:")
display(top_corr_reg)

# -------------------------------------------------------
# 8.2 Escalado
# -------------------------------------------------------
scaler_reg = StandardScaler()
Xr_train_scaled = scaler_reg.fit_transform(Xr_train)
Xr_test_scaled  = scaler_reg.transform(Xr_test)

# -------------------------------------------------------
# 8.3 PCA para regresión (95% varianza)
# -------------------------------------------------------
pca_reg_full = PCA(n_components=None, random_state=42)
pca_reg_full.fit(Xr_train_scaled)

var_exp_reg = pca_reg_full.explained_variance_ratio_
var_exp_reg_cum = np.cumsum(var_exp_reg)
n_comp_reg_95 = np.argmax(var_exp_reg_cum >= 0.95) + 1
print(f"\nComponentes PCA para regresión (~95% varianza): {n_comp_reg_95}")

# Tabla de varianza explicada por componente
tabla_pca_reg = pd.DataFrame({
    "Componente": np.arange(1, len(var_exp_reg) + 1),
    "Varianza explicada": var_exp_reg,
    "Varianza acumulada": var_exp_reg_cum
})
print("\nVarianza explicada por cada componente (regresión):")
display(tabla_pca_reg.head(10))  # primeras 10 filas para no saturar

# Gráfica de varianza acumulada
plt.figure(figsize=(6,4))
plt.plot(var_exp_reg_cum, marker="o")
plt.axhline(0.95, linestyle="--")
plt.xlabel("Número de componentes")
plt.ylabel("Varianza explicada acumulada")
plt.title("PCA – Varianza acumulada (regresión)")
plt.grid(True)
plt.show()

# PCA definitivo con n_comp_reg_95
pca_reg = PCA(n_components=n_comp_reg_95, random_state=42)
Xr_train_pca = pca_reg.fit_transform(Xr_train_scaled)
Xr_test_pca  = pca_reg.transform(Xr_test_scaled)

print("Xr_train_pca:", Xr_train_pca.shape, "Xr_test_pca:", Xr_test_pca.shape)

# -------------------------------------------------------
# 8.4 (Opcional) Visualización 2D de los primeros componentes
# -------------------------------------------------------
if n_comp_reg_95 >= 2:
    plt.figure(figsize=(6,5))
    scatter = plt.scatter(
        Xr_train_pca[:, 0],
        Xr_train_pca[:, 1],
        c=yr_train,
        cmap="viridis",
        alpha=0.7
    )
    plt.colorbar(scatter, label=target_reg)
    plt.xlabel("PC1")
    plt.ylabel("PC2")
    plt.title("Regresión – Proyección PCA (PC1 vs PC2)")
    plt.grid(True)
    plt.show()

""":<h1> 9. REGRESIÓN – KNN REGRESSOR + NN KERAS"""

print("\n=== 9. REGRESIÓN – MODELOS ===")

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Función auxiliar para métricas de regresión

def regression_metrics(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)
    return mae, mse, rmse, r2

# ---- 9.1 KNN REGRESSOR SIN PCA ----
knn_reg = KNeighborsRegressor(
    n_neighbors=5,
    weights="distance",
    metric="euclidean"
)
knn_reg.fit(Xr_train_scaled, yr_train)
yr_pred_knn = knn_reg.predict(Xr_test_scaled)
print("KNN Regressor SIN PCA entrenado.")

# ---- 9.2 KNN REGRESSOR CON PCA ----
knn_reg_pca = KNeighborsRegressor(
    n_neighbors=5,
    weights="distance",
    metric="euclidean"
)
knn_reg_pca.fit(Xr_train_pca, yr_train)
yr_pred_knn_pca = knn_reg_pca.predict(Xr_test_pca)
print("KNN Regressor CON PCA entrenado.")

# ---- 9.3 RED NEURONAL REGRESORA SIN PCA (KERAS) ----
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.callbacks import EarlyStopping

def build_nn_regressor(input_dim, n_hidden1=64, n_hidden2=32, lr=0.001, dropout_rate=0.2):
    model = Sequential()
    model.add(Input(shape=(input_dim,)))          # capa de entrada
    model.add(Dense(n_hidden1, activation="relu"))
    model.add(Dropout(dropout_rate))
    model.add(Dense(n_hidden2, activation="relu"))
    model.add(Dropout(dropout_rate))
    model.add(Dense(1, activation="linear"))      # salida continua
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
    model.compile(
        loss="mse",
        optimizer=optimizer,
        metrics=["mae"]
    )
    return model

early_stop_reg = EarlyStopping(
    monitor="val_loss",
    patience=10,
    restore_best_weights=True
)

input_dim_reg_full = Xr_train_scaled.shape[1]
nn_reg_full = build_nn_regressor(
    input_dim=input_dim_reg_full,
    n_hidden1=64,
    n_hidden2=32,
    lr=0.001,
    dropout_rate=0.2
)

history_reg_full = nn_reg_full.fit(
    Xr_train_scaled, yr_train,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[early_stop_reg],
    verbose=0
)

yr_pred_nn_full = nn_reg_full.predict(Xr_test_scaled).ravel()
print("Red neuronal regresora SIN PCA entrenada.")

# ---- 9.4 RED NEURONAL REGRESORA CON PCA (KERAS) ----
input_dim_reg_pca = Xr_train_pca.shape[1]
nn_reg_pca = build_nn_regressor(
    input_dim=input_dim_reg_pca,
    n_hidden1=32,
    n_hidden2=16,
    lr=0.001,
    dropout_rate=0.2
)

history_reg_pca = nn_reg_pca.fit(
    Xr_train_pca, yr_train,
    validation_split=0.2,
    epochs=200,
    batch_size=32,
    callbacks=[early_stop_reg],
    verbose=0
)

yr_pred_nn_pca = nn_reg_pca.predict(Xr_test_pca).ravel()
print("Red neuronal regresora CON PCA entrenada.")


# 10. GRÁFICAS REAL vs PREDICHO

plt.figure(figsize=(12,10))

model_names = ["KNN (sin PCA)", "KNN (con PCA)", "NN (sin PCA)", "NN (con PCA)"]
preds = [yr_pred_knn, yr_pred_knn_pca, yr_pred_nn_full, yr_pred_nn_pca]

for i, (name, y_pred) in enumerate(zip(model_names, preds), 1):
    plt.subplot(2, 2, i)
    plt.scatter(yr_test, y_pred, alpha=0.7)
    plt.plot([yr_test.min(), yr_test.max()],
             [yr_test.min(), yr_test.max()],
             "r--", linewidth=2)
    plt.xlabel("Valor real (mean area)")
    plt.ylabel("Valor predicho")
    plt.title(name)
    plt.grid(True)

plt.tight_layout()
plt.show()

"""<h1> Analisis de las graficas <br>

El análisis visual de los cuatro gráficos de dispersión revela que tanto el algoritmo de vecinos más cercanos (KNN) como las Redes Neuronales (NN) son altamente efectivos para la tarea de regresión de predecir el "mean area", demostrando una precisión excelente y una fuerte correlación lineal entre los valores reales y los predichos. Es notable que la inclusión del Análisis de Componentes Principales (PCA), una técnica común para la reducción de dimensionalidad, tuvo un impacto mínimo en el rendimiento de ambos modelos.

Con esto podemos concluir que las cuatro configuraciones evaluadas (KNN/NN con o sin PCA) son igualmente robustas y confiables para esta aplicación específica, lo que podría sugerir que la dimensionalidad original de los datos no representaba un problema significativo.

<h1> 10. MÉTRICAS DE REGRESIÓN
"""

# 10. MÉTRICAS Y TABLA DE RESULTADOS
print("\n=== 10. MÉTRICAS DE REGRESIÓN ===")

mae_knn, mse_knn, rmse_knn, r2_knn = regression_metrics(yr_test, yr_pred_knn)
mae_knn_pca, mse_knn_pca, rmse_knn_pca, r2_knn_pca = regression_metrics(yr_test, yr_pred_knn_pca)
mae_nn, mse_nn, rmse_nn, r2_nn = regression_metrics(yr_test, yr_pred_nn_full)
mae_nn_pca, mse_nn_pca, rmse_nn_pca, r2_nn_pca = regression_metrics(yr_test, yr_pred_nn_pca)

tabla_reg = pd.DataFrame({
    "Modelo": [
        "KNN (sin PCA)",
        "KNN (con PCA)",
        "NN (sin PCA)",
        "NN (con PCA)"
    ],
    "MAE":  [mae_knn, mae_knn_pca, mae_nn, mae_nn_pca],
    "MSE":  [mse_knn, mse_knn_pca, mse_nn, mse_nn_pca],
    "RMSE": [rmse_knn, rmse_knn_pca, rmse_nn, rmse_nn_pca],
    "R2":   [r2_knn, r2_knn_pca, r2_nn, r2_nn_pca]
})

print("\n=== TABLA DE MÉTRICAS DE REGRESIÓN ===")
display(tabla_reg)

"""<h1> 11. COMPARACIÓN RESUMIDA DE MODELOS CLASIFICACIÓN"""

print("\n=== 11. COMPARACIÓN RESUMIDA – CLASIFICACIÓN (Accuracy) ===")
res_clf = pd.DataFrame({
    "Modelo": ["KNN", "KNN + PCA", "NN Keras", "NN Keras + PCA"],
    "Accuracy": [acc_knn, acc_knn_pca, acc_nn_full, acc_nn_pca]
})
display(res_clf)

"""<h1> 13. PREDICCIÓN CON NUEVOS EJEMPLOS

"""

print("\n=== 13. PREDICCIÓN CON NUEVOS EJEMPLOS ===")

# Un ejemplo benigno (target = 1) y uno maligno (target = 0) del test
ejemplo_benigno = Xc_test[yc_test == 1].iloc[0:1]
ejemplo_maligno = Xc_test[yc_test == 0].iloc[0:1]

print("\nEjemplo benigno (clase real = 1):")
display(ejemplo_benigno)

print("\nEjemplo maligno (clase real = 0):")
display(ejemplo_maligno)

def predecir_ejemplo(df_ejemplo, scaler, modelo_knn, modelo_nn):
    """
    df_ejemplo : DataFrame con UNA fila (un paciente)
    scaler     : StandardScaler ya ajustado en entrenamiento
    modelo_knn : mejor modelo KNN de clasificación
    modelo_nn  : red neuronal de clasificación (salida sigmoide)
    """
    # Escalamos igual que en entrenamiento
    x_scaled = scaler.transform(df_ejemplo)

    # Predicción KNN (clase 0/1)
    pred_knn = int(modelo_knn.predict(x_scaled)[0])

    # Red neuronal devuelve probabilidad de clase 1 (BENIGNO)
    proba_benigno = float(modelo_nn.predict(x_scaled).ravel()[0])
    pred_nn = int(proba_benigno >= 0.5)

    return pred_knn, pred_nn, proba_benigno

# --------------------- Predicción para ejemplo BENIGNO (modelos sin PCA) ---
pk_knn_b, pk_nn_b, proba_b_benigno = predecir_ejemplo(
    ejemplo_benigno, scaler_clf, best_knn_clf, nn_clf_full
)

print("\n--- Predicción para ejemplo BENIGNO (modelos sin PCA) ---")
print(f"KNN predice clase        : {pk_knn_b}")
print(f"Red neuronal predice clase: {pk_nn_b}")
print(f"Prob. BENIGNO (NN)        : {proba_b_benigno:.4f}")
print(f"Prob. MALIGNO (NN)        : {1 - proba_b_benigno:.4f}")

# ------------------- Predicción para ejemplo MALIGNO (modelos sin PCA) ---
pk_knn_m, pk_nn_m, proba_m_benigno = predecir_ejemplo(
    ejemplo_maligno, scaler_clf, best_knn_clf, nn_clf_full
)

print("\n--- Predicción para ejemplo MALIGNO (modelos sin PCA) ---")
print(f"KNN predice clase        : {pk_knn_m}")
print(f"Red neuronal predice clase: {pk_nn_m}")
print(f"Prob. BENIGNO (NN)        : {proba_m_benigno:.4f}")
print(f"Prob. MALIGNO (NN)        : {1 - proba_m_benigno:.4f}")



# 13.2 Ejemplos SINTÉTICOS usando las variables MÁS CORRELACIONADAS

print("\n=== 13.2 PREDICCIÓN CON EJEMPLOS SINTÉTICOS ===")
print("Usando las variables más correlacionadas y completando el resto con medias.")

# Partimos de un "template" con la media de cada variable
template_medias = Xc_train.mean()

# Ejemplo 1: Tumor pequeño (probablemente benigno)
ej_sint_benigno = template_medias.copy()
ej_sint_benigno.loc["worst concave points"] = 0.05
ej_sint_benigno.loc["worst perimeter"]      = 80.0
ej_sint_benigno.loc["mean concave points"]  = 0.03
ej_sint_benigno.loc["worst radius"]         = 12.0
ej_sint_benigno.loc["worst area"]           = 400.0

# Ejemplo 2: Tumor grande e irregular (probablemente maligno)
ej_sint_maligno = template_medias.copy()
ej_sint_maligno.loc["worst concave points"] = 0.25
ej_sint_maligno.loc["worst perimeter"]      = 150.0
ej_sint_maligno.loc["mean concave points"]  = 0.15
ej_sint_maligno.loc["worst radius"]         = 25.0
ej_sint_maligno.loc["worst area"]           = 1500.0

# Convertimos a DataFrame con el mismo orden de columnas que Xc_train
nuevo_benigno = pd.DataFrame([ej_sint_benigno])[Xc_train.columns]
nuevo_maligno = pd.DataFrame([ej_sint_maligno])[Xc_train.columns]

print("\nEjemplo sintético BENIGNO (valores fijados + medias):")
display(nuevo_benigno)

print("\nEjemplo sintético MALIGNO (valores fijados + medias):")
display(nuevo_maligno)

# Predicciones para los ejemplos sintéticos
pk_knn_nb, pk_nn_nb, proba_nb_benigno = predecir_ejemplo(
    nuevo_benigno, scaler_clf, best_knn_clf, nn_clf_full
)

print("\n--- Predicción para ejemplo SINTÉTICO BENIGNO ---")
print(f"KNN predice clase        : {pk_knn_nb}")
print(f"Red neuronal predice clase: {pk_nn_nb}")
print(f"Prob. BENIGNO (NN)        : {proba_nb_benigno:.4f}")
print(f"Prob. MALIGNO (NN)        : {1 - proba_nb_benigno:.4f}")

pk_knn_nm, pk_nn_nm, proba_nm_benigno = predecir_ejemplo(
    nuevo_maligno, scaler_clf, best_knn_clf, nn_clf_full
)

print("\n--- Predicción para ejemplo SINTÉTICO MALIGNO ---")
print(f"KNN predice clase        : {pk_knn_nm}")
print(f"Red neuronal predice clase: {pk_nn_nm}")
print(f"Prob. BENIGNO (NN)        : {proba_nm_benigno:.4f}")
print(f"Prob. MALIGNO (NN)        : {1 - proba_nm_benigno:.4f}")

"""<h1> CONCLUSION<br>

El estudio realizado sobre el dataset Breast Cancer Wisconsin permitió evaluar el impacto del Análisis de Componentes Principales (PCA) y la efectividad del algoritmo KNN en la detección de cáncer de mama. En primera instancia, el análisis sin reducción de dimensionalidad mostró un desempeño sólido, reflejado en una alta capacidad del modelo para diferenciar entre tumores benignos y malignos. Sin embargo, al aplicar PCA se obtuvo un modelo más compacto y eficiente, manteniendo un rendimiento similar y evidenciando que gran parte de la información relevante del conjunto original puede ser preservada con menos componentes.

Las matrices de confusión mostraron que con y sin PCA se mantiene una alta tasa de verdaderos positivos y verdaderos negativos, con ligeras variaciones en los errores de clasificación. En conjunto, estos resultados indican que PCA no solo reduce la complejidad del modelo, sino que también contribuye a disminuir el ruido y la redundancia entre variables, sin comprometer significativamente la precisión general.

En conclusión, la combinación de PCA y KNN constituye un enfoque adecuado y eficiente para este tipo de problemas de diagnóstico, permitiendo construir modelos más ligeros, interpretables y competitivos. Este análisis reafirma la utilidad de técnicas de aprendizaje supervisado junto con métodos de reducción de dimensionalidad para abordar problemas clínicos donde la correcta clasificación es fundamental para la toma de decisiones médicas.

REFERENCIAS

* Street, W. N., Wolberg, W. H., & Mangasarian, O. L. (1993). Nuclear feature extraction for breast tumor diagnosis. University of Wisconsin.

* Wolberg, W. H., & Mangasarian, O. L. (1990). Multisurface method of pattern separation for medical diagnosis applied to breast cytology. Proceedings of the National Academy of Sciences, 87(23), 9193–9196.

* Bennett, K. P., & Mangasarian, O. L. (1992). Robust linear programming discrimination of two linearly inseparable sets. Optimization Methods and Software, 1(1), 23–34.

* Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., … Duchesnay, É. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825–2830.

* Müller, A. C., & Guido, S. (2016). Introduction to Machine Learning with Python: A Guide for Data Scientists. O’Reilly Media.

Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction (2nd ed.). Springer.

* James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning. Springer.

* Kuhn, M., & Johnson, K. (2013). Applied Predictive Modeling. Springer.

* Jolliffe, I. T., & Cadima, J. (2016). Principal component analysis: A review and recent developments. Philosophical Transactions of the Royal Society A, 374(2065).

* Abadi, M., Barham, P., Chen, J., et al. (2015). TensorFlow: Large-scale machine learning on heterogeneous systems. Google Research.

* Chollet, F. (2015). Keras. GitHub repository. https://github.com/fchollet/keras


Tibshirani, R. (1996). Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society.

Breiman, L. (2001). Random forests. Machine Learning, 45(1), 5–32.

Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.

Lundberg, S. M., & Lee, S.-I. (2017). A unified approach to interpreting model predictions (SHAP). Advances in Neural Information Processing Systems.

Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). “Why Should I Trust You?”: Explaining the Predictions of Any Classifier (LIME). ACM SIGKDD.

Esteva, A., Kuprel, B., Novoa, R. A., et al. (2017). Dermatologist-level classification of skin cancer with deep neural networks. Nature, 542, 115–118.

Litjens, G., Kooi, T., Bejnordi, B. E., et al. (2017). A survey on deep learning in medical image analysis. Medical Image Analysis, 42, 60–88.

Chicco, D., & Jurman, G. (2020). The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation. BMC Genomics, 21(1), 1–13.

Powers, D. M. W. (2011). Evaluation: From precision, recall and F-measure to ROC, informedness, markedness & correlation. Journal of Machine Learning Technologies.

Sokolova, M., & Lapalme, G. (2009). A systematic analysis of performance measures for classification tasks. Information Processing & Management.

Zhang, Z. (2016). Introduction to machine learning: k-nearest neighbors. Annals of Translational Medicine, 4(11).

Srivastava, N., Hinton, G., Krizhevsky, A., et al. (2014). Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research.

Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning

representations by back-propagating errors. Nature.

Kohavi, R. (1995). A study of cross-validation and bootstrap for accuracy estimation and model selection. IJCAI.
"""